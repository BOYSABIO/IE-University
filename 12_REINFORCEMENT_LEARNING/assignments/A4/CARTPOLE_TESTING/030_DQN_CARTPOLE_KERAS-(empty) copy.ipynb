{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb89376",
   "metadata": {},
   "source": [
    "### **CARTPOLE DQN KERAS**\n",
    "\n",
    "This is a naive version of the CARTPOLE DQN algorithm <br>\n",
    "Try to fill the missing parts\n",
    "\n",
    "This is the simplest approach of DQN with a single network (Remember the standard DQN uses 2 networks and trains one, in this one we use one and train one\n",
    "\n",
    "Fill the empty places and try to make it to work Ask for help if stocked!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557b1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use mixed-precision training for faster computations on supported GPUs\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "import sys\n",
    "sys.stderr = open('err.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2041146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "gpu_devices = [device for device in devices if device.device_type == 'GPU']\n",
    "for gpu in gpu_devices:\n",
    "    print('Using', gpu.physical_device_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d89820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2be125",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78010280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to fine tune\n",
    "# Try your own parameters\n",
    "# Remember epsilon and gamma are very important\n",
    "\n",
    "\n",
    "MAX_EPISODES = 300\n",
    "ROLLING_WINDOW = 20\n",
    "MEMORY_SIZE = 2000\n",
    "MAX_STEPS = 500\n",
    "\n",
    "gamma = 0.99                        # discount rate\n",
    "epsilon = 1.0                        # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "solved_threshold = 195\n",
    "\n",
    "verb = 0                             # to see traces (verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d058e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa84c05",
   "metadata": {},
   "source": [
    "#### **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5799f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# [CREATE YOUR NEURAL NETWORK TRY 16/32 or 24/24]\n",
    "###\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,), name=\"state_input\")\n",
    "    x = Dense(16, activation='relu', name=\"dense_1\")(inputs)\n",
    "    x = Dense(32, activation='relu', name=\"dense_2\")(x)\n",
    "    outputs = Dense(action_size, activation='linear', name=\"output_layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Q_Network\")\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fca383",
   "metadata": {},
   "source": [
    "#### **Support Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761e2bb",
   "metadata": {},
   "source": [
    "$Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') \\cdot (1 - \\text{done})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c07fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "def store(state, action, reward, next_state, done):             \n",
    "    \"\"\"\n",
    "    This function appends the actual observation and reward to the Replay buffer\n",
    "    \"\"\"\n",
    "    state = state.squeeze()\n",
    "    next_state = next_state.squeeze()\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def select_action_greedy(state, model, epsilon):           \n",
    "    \"\"\"\n",
    "    Selects action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    act_values = model.predict(state, verbose=verb)\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "# Sample experiences from the replay buffer\n",
    "def sample_experiences(batch_size):\n",
    "    \"\"\"\n",
    "    Samples a batch_size of experiences from the Replay buffer. \n",
    "    You MUST transform the data into numpy arrays as this accelerates the response time sensibily\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_states = np.array(next_states)\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "###\n",
    "# YOU MUST VECTIORIZE THE RETURN. TRANSFORM THE OUTPUTS IN np arrays otherwise it will be very slow\n",
    "###\n",
    "\n",
    "    \n",
    "def experience_replay(batch_size, model, epsilon):\n",
    "    \"\"\"\n",
    "    The critical function in the whole program\n",
    "    1. gets a minibatch from replay buffer\n",
    "    2. Predicts targets_qs from states (we need the full batch predicted but we'll avoid dones)\n",
    "    3. predicts next_qs from next_states\n",
    "    4. Bellman equation on next_qs obtains new target_qs $Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') \\cdot (1 - \\text{done})$\n",
    "\n",
    "    6. Train DQN input states, output target_qs \n",
    "\n",
    "    \"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    \n",
    "    # Predict Q-values for current and next states using vectorized operations\n",
    "\n",
    "    \n",
    "#### \n",
    "#  [ Predict target_qs with model predict states, predict next_qs with model predict next states]\n",
    "####\n",
    "    target_qs = model.predict(states, verbose=verb) # Get current Q-values for all states\n",
    "    next_qs = model.predict(next_states, verbose=verb) # Get next Q-values for next states\n",
    "\n",
    "    # Update target Q-values using standard DQN logic   \n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * np.max(next_qs, axis=1) * (1 - dones)\n",
    "    \n",
    "    # Train the model on the Q-values\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "###\n",
    "#[Here you have a load and save weights. Can you make a breakpoint to restart the program after a while of training?]\n",
    "###\n",
    "\n",
    "def load(name, DQN):\n",
    "    DQN.load_weights(name)\n",
    "\n",
    "def save(name, DQN):\n",
    "    DQN.save_weights(name)\n",
    "\n",
    "DQN = build_model(state_size, action_size)\n",
    "DQN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loop for DQN\n",
    "rewards_per_episode = []\n",
    "rolling_avg_rewards = []\n",
    "rolling_avg = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action_greedy(state, DQN, epsilon)  # Updated to pass epsilon\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            experience_replay(batch_size, DQN, epsilon)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Track rewards\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "    rolling_avg_rewards.append(rolling_avg)\n",
    "    \n",
    "    print(f\"Episode: {episode+1:3}/{MAX_EPISODES}, Reward: {total_reward:+7.2f}, \"\n",
    "          f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:6.2f}, Steps: {step:3}\")\n",
    "    \n",
    "    # Check if environment is solved\n",
    "    if rolling_avg >= solved_threshold:\n",
    "        print(f\"Environment solved in {episode+1} episodes!\")\n",
    "        DQN.save(\"CartPole_dqn_model.keras\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Training completed in {training_duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419d918",
   "metadata": {},
   "source": [
    "#### **Learning Plot and Episode Rewards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards with rolling average\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_per_episode, label='Rewards', color='blue')\n",
    "plt.plot(rolling_avg_rewards, label='Rolling Avg (Last 20 Episodes)', color='orange')\n",
    "plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "plt.title('DQN Training Performance CARTPOLE')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baef8f",
   "metadata": {},
   "source": [
    "#### **Simulation - Testing 10 episodes with the DQN Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 10 episodes\n",
    "start_time = time.time()\n",
    "\n",
    "for e_test in range(10):  # Run 10 test episodes\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    \n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the trained model for testing\n",
    "        action_vals = DQN.predict(state, verbose=0)  # Predict action values\n",
    "        action = np.argmax(action_vals[0])  # Choose the action with the highest Q-value\n",
    "\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps = steps + 1\n",
    "        if done or truncated:\n",
    "            print(f\"Test Episode: {e_test + 1:2}/10, Reward: {total_reward:.2f}, Steps: {steps:3}\")\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "testing_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Testing completed in {testing_duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f36b76",
   "metadata": {},
   "source": [
    "#### **Rendering 1 episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent with video rendering\n",
    "# This code is useful if you are using colab otherwise use render_mode='human'\n",
    "env = gym.make((\"CartPole-v1\"), render_mode='rgb_array')  # Enable RGB rendering\n",
    "frames = []  # Store frames for visualization\n",
    "\n",
    "# Render a single test episode\n",
    "state, _ = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "tot_rewards = 0\n",
    "\n",
    "while True:\n",
    "    # Use the trained model for action\n",
    "    action_vals = DQN.predict(state, verbose=0)  # Predict action values\n",
    "    action = np.argmax(action_vals[0])           # Choose the action with the highest Q-value\n",
    "\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    frames.append(env.render())                  # Save frame for rendering later\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    tot_rewards += reward\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        print(f\"Rendered Test Episode Reward: {tot_rewards:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save the rendered episode as a GIF\n",
    "def save_frames_as_gif(frames, path='./', filename='CARTPOLE_DQN.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    gif_path = os.path.join(path, filename)\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=50, loop=0)\n",
    "    print(f\"Saved GIF to: {gif_path}\")\n",
    "\n",
    "save_frames_as_gif(frames, filename='CARTPOLE_DQN.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebfabb",
   "metadata": {},
   "source": [
    "## CartPole DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDQN Cartpole\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,), name=\"state_input\")\n",
    "    x = Dense(24, activation='relu', name=\"dense_1\")(inputs)\n",
    "    x = Dense(24, activation='relu', name=\"dense_2\")(x)\n",
    "    outputs = Dense(action_size, activation='linear', name=\"output_layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Q_Network\")\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Create main and target networks\n",
    "DQN = build_model(state_size, action_size)\n",
    "target_DQN = build_model(state_size, action_size)\n",
    "target_DQN.set_weights(DQN.get_weights()) # Initialize target network with main network weights\n",
    "\n",
    "def update_target_network():\n",
    "    \"\"\"\n",
    "    Update the target network with the main network weights\n",
    "    \"\"\"\n",
    "    target_DQN.set_weights(DQN.get_weights())\n",
    "\n",
    "def select_action_greedy(state, model, epsilon):\n",
    "    \"\"\"\n",
    "    Selects action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    act_values = model.predict(state, verbose=verb)\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "def experience_replay(batch_size, model, epsilon):\n",
    "    \"\"\"\n",
    "    Sample a batch of experiences from the replay buffer.\n",
    "    Double DQN: Use the main network to select the action, and the target network to evaluate the Q-values.\n",
    "    \"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "\n",
    "    # Get current Q-values from the main network\n",
    "    target_qs = model.predict(states, verbose=verb)\n",
    "\n",
    "    # Double DQN: Use the main network to select the action, and the target network to evaluate the Q-values\n",
    "    next_qs_main = model.predict(next_states, verbose=verb)  # Main network predicts next states\n",
    "    next_qs_target = target_DQN.predict(next_states, verbose=verb)  # Target network predicts next states\n",
    "\n",
    "    # Select the best action for each next state using the main network\n",
    "    best_actions = np.argmax(next_qs_main, axis=1)\n",
    "\n",
    "    # Get Q-values for the best actions from the target network\n",
    "    best_qs = next_qs_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "    # Update the target Q-values using the Bellman equation\n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * best_qs * (1 - dones)\n",
    "\n",
    "    # Train the main network on the target Q-values\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "# Training loop for DDQN\n",
    "rewards_per_episode = []\n",
    "rolling_avg_rewards = []\n",
    "rolling_avg = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action_greedy(state, DQN, epsilon)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            experience_replay(batch_size, DQN, epsilon)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Update target network every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        update_target_network()\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Track rewards\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "    rolling_avg_rewards.append(rolling_avg)\n",
    "    \n",
    "    print(f\"Episode: {episode+1:3}/{MAX_EPISODES}, Reward: {total_reward:+7.2f}, \"\n",
    "          f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:6.2f}, Steps: {step:3}\")\n",
    "    \n",
    "    # Check if environment is solved\n",
    "    if rolling_avg >= solved_threshold:\n",
    "        print(f\"Environment solved in {episode+1} episodes!\")\n",
    "        DQN.save(\"CartPole_ddqn_model.keras\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Training completed in {training_duration:.2f} minutes\")\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_per_episode, label='Rewards', color='blue')\n",
    "plt.plot(rolling_avg_rewards, label='Rolling Avg (Last 20 Episodes)', color='orange')\n",
    "plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "plt.title('DDQN Training Performance CARTPOLE')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260156ee",
   "metadata": {},
   "source": [
    "## Testing Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64a5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: DQN_16-32\n",
      "Config: DQN_16-32, Ep:   1/300, Reward:   10.00, Epsilon: 0.99, Rolling Avg:  10.00\n",
      "Config: DQN_16-32, Ep:   2/300, Reward:   22.00, Epsilon: 0.98, Rolling Avg:  16.00\n",
      "Config: DQN_16-32, Ep:   3/300, Reward:   34.00, Epsilon: 0.97, Rolling Avg:  22.00\n",
      "Config: DQN_16-32, Ep:   4/300, Reward:   23.00, Epsilon: 0.96, Rolling Avg:  22.25\n",
      "Config: DQN_16-32, Ep:   5/300, Reward:   46.00, Epsilon: 0.95, Rolling Avg:  27.00\n",
      "Config: DQN_16-32, Ep:   6/300, Reward:   21.00, Epsilon: 0.94, Rolling Avg:  26.00\n",
      "Config: DQN_16-32, Ep:   7/300, Reward:   20.00, Epsilon: 0.93, Rolling Avg:  25.14\n",
      "Config: DQN_16-32, Ep:   8/300, Reward:   16.00, Epsilon: 0.92, Rolling Avg:  24.00\n",
      "Config: DQN_16-32, Ep:   9/300, Reward:   18.00, Epsilon: 0.91, Rolling Avg:  23.33\n",
      "Config: DQN_16-32, Ep:  10/300, Reward:   26.00, Epsilon: 0.90, Rolling Avg:  23.60\n",
      "Config: DQN_16-32, Ep:  11/300, Reward:   11.00, Epsilon: 0.90, Rolling Avg:  22.45\n",
      "Config: DQN_16-32, Ep:  12/300, Reward:   13.00, Epsilon: 0.89, Rolling Avg:  21.67\n",
      "Config: DQN_16-32, Ep:  13/300, Reward:   15.00, Epsilon: 0.88, Rolling Avg:  21.15\n",
      "Config: DQN_16-32, Ep:  14/300, Reward:   12.00, Epsilon: 0.87, Rolling Avg:  20.50\n",
      "Config: DQN_16-32, Ep:  15/300, Reward:   15.00, Epsilon: 0.86, Rolling Avg:  20.13\n",
      "Config: DQN_16-32, Ep:  16/300, Reward:   23.00, Epsilon: 0.85, Rolling Avg:  20.31\n",
      "Config: DQN_16-32, Ep:  17/300, Reward:   17.00, Epsilon: 0.84, Rolling Avg:  20.12\n",
      "Config: DQN_16-32, Ep:  18/300, Reward:   11.00, Epsilon: 0.83, Rolling Avg:  19.61\n",
      "Config: DQN_16-32, Ep:  19/300, Reward:   22.00, Epsilon: 0.83, Rolling Avg:  19.74\n",
      "Config: DQN_16-32, Ep:  20/300, Reward:   25.00, Epsilon: 0.82, Rolling Avg:  20.00\n",
      "Config: DQN_16-32, Ep:  21/300, Reward:   14.00, Epsilon: 0.81, Rolling Avg:  20.20\n",
      "Config: DQN_16-32, Ep:  22/300, Reward:   31.00, Epsilon: 0.80, Rolling Avg:  20.65\n",
      "Config: DQN_16-32, Ep:  23/300, Reward:   23.00, Epsilon: 0.79, Rolling Avg:  20.10\n",
      "Config: DQN_16-32, Ep:  24/300, Reward:   36.00, Epsilon: 0.79, Rolling Avg:  20.75\n",
      "Config: DQN_16-32, Ep:  25/300, Reward:   37.00, Epsilon: 0.78, Rolling Avg:  20.30\n",
      "Config: DQN_16-32, Ep:  26/300, Reward:   18.00, Epsilon: 0.77, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  27/300, Reward:   20.00, Epsilon: 0.76, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  28/300, Reward:   17.00, Epsilon: 0.75, Rolling Avg:  20.20\n",
      "Config: DQN_16-32, Ep:  29/300, Reward:   14.00, Epsilon: 0.75, Rolling Avg:  20.00\n",
      "Config: DQN_16-32, Ep:  30/300, Reward:   12.00, Epsilon: 0.74, Rolling Avg:  19.30\n",
      "Config: DQN_16-32, Ep:  31/300, Reward:   23.00, Epsilon: 0.73, Rolling Avg:  19.90\n",
      "Config: DQN_16-32, Ep:  32/300, Reward:   18.00, Epsilon: 0.72, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  33/300, Reward:   11.00, Epsilon: 0.72, Rolling Avg:  19.95\n",
      "Config: DQN_16-32, Ep:  34/300, Reward:   27.00, Epsilon: 0.71, Rolling Avg:  20.70\n",
      "Config: DQN_16-32, Ep:  35/300, Reward:    9.00, Epsilon: 0.70, Rolling Avg:  20.40\n",
      "Config: DQN_16-32, Ep:  36/300, Reward:   13.00, Epsilon: 0.70, Rolling Avg:  19.90\n",
      "Config: DQN_16-32, Ep:  37/300, Reward:   24.00, Epsilon: 0.69, Rolling Avg:  20.25\n",
      "Config: DQN_16-32, Ep:  38/300, Reward:   32.00, Epsilon: 0.68, Rolling Avg:  21.30\n",
      "Config: DQN_16-32, Ep:  39/300, Reward:   12.00, Epsilon: 0.68, Rolling Avg:  20.80\n",
      "Config: DQN_16-32, Ep:  40/300, Reward:   25.00, Epsilon: 0.67, Rolling Avg:  20.80\n",
      "Config: DQN_16-32, Ep:  41/300, Reward:   11.00, Epsilon: 0.66, Rolling Avg:  20.65\n",
      "Config: DQN_16-32, Ep:  42/300, Reward:   17.00, Epsilon: 0.66, Rolling Avg:  19.95\n",
      "Config: DQN_16-32, Ep:  43/300, Reward:   24.00, Epsilon: 0.65, Rolling Avg:  20.00\n",
      "Config: DQN_16-32, Ep:  44/300, Reward:   15.00, Epsilon: 0.64, Rolling Avg:  18.95\n",
      "Config: DQN_16-32, Ep:  45/300, Reward:   24.00, Epsilon: 0.64, Rolling Avg:  18.30\n",
      "Config: DQN_16-32, Ep:  46/300, Reward:    9.00, Epsilon: 0.63, Rolling Avg:  17.85\n",
      "Config: DQN_16-32, Ep:  47/300, Reward:   21.00, Epsilon: 0.62, Rolling Avg:  17.90\n",
      "Config: DQN_16-32, Ep:  48/300, Reward:   15.00, Epsilon: 0.62, Rolling Avg:  17.80\n",
      "Config: DQN_16-32, Ep:  49/300, Reward:   11.00, Epsilon: 0.61, Rolling Avg:  17.65\n",
      "Config: DQN_16-32, Ep:  50/300, Reward:   10.00, Epsilon: 0.61, Rolling Avg:  17.55\n",
      "Config: DQN_16-32, Ep:  51/300, Reward:   13.00, Epsilon: 0.60, Rolling Avg:  17.05\n",
      "Config: DQN_16-32, Ep:  52/300, Reward:   12.00, Epsilon: 0.59, Rolling Avg:  16.75\n",
      "Config: DQN_16-32, Ep:  53/300, Reward:   19.00, Epsilon: 0.59, Rolling Avg:  17.15\n",
      "Config: DQN_16-32, Ep:  54/300, Reward:   18.00, Epsilon: 0.58, Rolling Avg:  16.70\n",
      "Config: DQN_16-32, Ep:  55/300, Reward:   11.00, Epsilon: 0.58, Rolling Avg:  16.80\n",
      "Config: DQN_16-32, Ep:  56/300, Reward:   12.00, Epsilon: 0.57, Rolling Avg:  16.75\n",
      "Config: DQN_16-32, Ep:  57/300, Reward:   10.00, Epsilon: 0.56, Rolling Avg:  16.05\n",
      "Config: DQN_16-32, Ep:  58/300, Reward:   23.00, Epsilon: 0.56, Rolling Avg:  15.60\n",
      "Config: DQN_16-32, Ep:  59/300, Reward:   36.00, Epsilon: 0.55, Rolling Avg:  16.80\n",
      "Config: DQN_16-32, Ep:  60/300, Reward:   42.00, Epsilon: 0.55, Rolling Avg:  17.65\n",
      "Config: DQN_16-32, Ep:  61/300, Reward:   49.00, Epsilon: 0.54, Rolling Avg:  19.55\n",
      "Config: DQN_16-32, Ep:  62/300, Reward:   29.00, Epsilon: 0.54, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  63/300, Reward:   15.00, Epsilon: 0.53, Rolling Avg:  19.70\n",
      "Config: DQN_16-32, Ep:  64/300, Reward:   22.00, Epsilon: 0.53, Rolling Avg:  20.05\n",
      "Config: DQN_16-32, Ep:  65/300, Reward:   16.00, Epsilon: 0.52, Rolling Avg:  19.65\n",
      "Config: DQN_16-32, Ep:  66/300, Reward:   15.00, Epsilon: 0.52, Rolling Avg:  19.95\n",
      "Config: DQN_16-32, Ep:  67/300, Reward:   34.00, Epsilon: 0.51, Rolling Avg:  20.60\n",
      "Config: DQN_16-32, Ep:  68/300, Reward:   33.00, Epsilon: 0.50, Rolling Avg:  21.50\n",
      "Config: DQN_16-32, Ep:  69/300, Reward:   18.00, Epsilon: 0.50, Rolling Avg:  21.85\n",
      "Config: DQN_16-32, Ep:  70/300, Reward:   39.00, Epsilon: 0.49, Rolling Avg:  23.30\n",
      "Config: DQN_16-32, Ep:  71/300, Reward:   11.00, Epsilon: 0.49, Rolling Avg:  23.20\n",
      "Config: DQN_16-32, Ep:  72/300, Reward:   18.00, Epsilon: 0.48, Rolling Avg:  23.50\n",
      "Config: DQN_16-32, Ep:  73/300, Reward:   21.00, Epsilon: 0.48, Rolling Avg:  23.60\n",
      "Config: DQN_16-32, Ep:  74/300, Reward:   45.00, Epsilon: 0.48, Rolling Avg:  24.95\n",
      "Config: DQN_16-32, Ep:  75/300, Reward:   14.00, Epsilon: 0.47, Rolling Avg:  25.10\n",
      "Config: DQN_16-32, Ep:  76/300, Reward:   18.00, Epsilon: 0.47, Rolling Avg:  25.40\n",
      "Config: DQN_16-32, Ep:  77/300, Reward:   12.00, Epsilon: 0.46, Rolling Avg:  25.50\n",
      "Config: DQN_16-32, Ep:  78/300, Reward:   13.00, Epsilon: 0.46, Rolling Avg:  25.00\n",
      "Config: DQN_16-32, Ep:  79/300, Reward:   19.00, Epsilon: 0.45, Rolling Avg:  24.15\n",
      "Config: DQN_16-32, Ep:  80/300, Reward:   22.00, Epsilon: 0.45, Rolling Avg:  23.15\n",
      "Config: DQN_16-32, Ep:  81/300, Reward:   28.00, Epsilon: 0.44, Rolling Avg:  22.10\n",
      "Config: DQN_16-32, Ep:  82/300, Reward:   64.00, Epsilon: 0.44, Rolling Avg:  23.85\n",
      "Config: DQN_16-32, Ep:  83/300, Reward:   24.00, Epsilon: 0.43, Rolling Avg:  24.30\n",
      "Config: DQN_16-32, Ep:  84/300, Reward:   23.00, Epsilon: 0.43, Rolling Avg:  24.35\n",
      "Config: DQN_16-32, Ep:  85/300, Reward:   26.00, Epsilon: 0.43, Rolling Avg:  24.85\n",
      "Config: DQN_16-32, Ep:  86/300, Reward:   24.00, Epsilon: 0.42, Rolling Avg:  25.30\n",
      "Config: DQN_16-32, Ep:  87/300, Reward:   19.00, Epsilon: 0.42, Rolling Avg:  24.55\n",
      "Config: DQN_16-32, Ep:  88/300, Reward:   23.00, Epsilon: 0.41, Rolling Avg:  24.05\n",
      "Config: DQN_16-32, Ep:  89/300, Reward:   18.00, Epsilon: 0.41, Rolling Avg:  24.05\n",
      "Config: DQN_16-32, Ep:  90/300, Reward:   25.00, Epsilon: 0.40, Rolling Avg:  23.35\n",
      "Config: DQN_16-32, Ep:  91/300, Reward:   43.00, Epsilon: 0.40, Rolling Avg:  24.95\n",
      "Config: DQN_16-32, Ep:  92/300, Reward:   34.00, Epsilon: 0.40, Rolling Avg:  25.75\n",
      "Config: DQN_16-32, Ep:  93/300, Reward:   24.00, Epsilon: 0.39, Rolling Avg:  25.90\n",
      "Config: DQN_16-32, Ep:  94/300, Reward:   21.00, Epsilon: 0.39, Rolling Avg:  24.70\n",
      "Config: DQN_16-32, Ep:  95/300, Reward:   19.00, Epsilon: 0.38, Rolling Avg:  24.95\n",
      "Config: DQN_16-32, Ep:  96/300, Reward:   14.00, Epsilon: 0.38, Rolling Avg:  24.75\n",
      "Config: DQN_16-32, Ep:  97/300, Reward:   31.00, Epsilon: 0.38, Rolling Avg:  25.70\n",
      "Config: DQN_16-32, Ep:  98/300, Reward:   29.00, Epsilon: 0.37, Rolling Avg:  26.50\n",
      "Config: DQN_16-32, Ep:  99/300, Reward:   35.00, Epsilon: 0.37, Rolling Avg:  27.30\n",
      "Config: DQN_16-32, Ep: 100/300, Reward:   19.00, Epsilon: 0.37, Rolling Avg:  27.15\n",
      "Config: DQN_16-32, Ep: 101/300, Reward:   16.00, Epsilon: 0.36, Rolling Avg:  26.55\n",
      "Config: DQN_16-32, Ep: 102/300, Reward:   17.00, Epsilon: 0.36, Rolling Avg:  24.20\n",
      "Config: DQN_16-32, Ep: 103/300, Reward:   17.00, Epsilon: 0.36, Rolling Avg:  23.85\n",
      "Config: DQN_16-32, Ep: 104/300, Reward:   17.00, Epsilon: 0.35, Rolling Avg:  23.55\n",
      "Config: DQN_16-32, Ep: 105/300, Reward:   33.00, Epsilon: 0.35, Rolling Avg:  23.90\n",
      "Config: DQN_16-32, Ep: 106/300, Reward:   24.00, Epsilon: 0.34, Rolling Avg:  23.90\n",
      "Config: DQN_16-32, Ep: 107/300, Reward:   22.00, Epsilon: 0.34, Rolling Avg:  24.05\n",
      "Config: DQN_16-32, Ep: 108/300, Reward:   19.00, Epsilon: 0.34, Rolling Avg:  23.85\n",
      "Config: DQN_16-32, Ep: 109/300, Reward:   38.00, Epsilon: 0.33, Rolling Avg:  24.85\n",
      "Config: DQN_16-32, Ep: 110/300, Reward:   33.00, Epsilon: 0.33, Rolling Avg:  25.25\n",
      "Config: DQN_16-32, Ep: 111/300, Reward:   21.00, Epsilon: 0.33, Rolling Avg:  24.15\n",
      "Config: DQN_16-32, Ep: 112/300, Reward:   26.00, Epsilon: 0.32, Rolling Avg:  23.75\n",
      "Config: DQN_16-32, Ep: 113/300, Reward:   29.00, Epsilon: 0.32, Rolling Avg:  24.00\n",
      "Config: DQN_16-32, Ep: 114/300, Reward:   28.00, Epsilon: 0.32, Rolling Avg:  24.35\n",
      "Config: DQN_16-32, Ep: 115/300, Reward:   42.00, Epsilon: 0.31, Rolling Avg:  25.50\n",
      "Config: DQN_16-32, Ep: 116/300, Reward:   61.00, Epsilon: 0.31, Rolling Avg:  27.85\n",
      "Config: DQN_16-32, Ep: 117/300, Reward:   45.00, Epsilon: 0.31, Rolling Avg:  28.55\n",
      "Config: DQN_16-32, Ep: 118/300, Reward:   20.00, Epsilon: 0.31, Rolling Avg:  28.10\n",
      "Config: DQN_16-32, Ep: 119/300, Reward:   25.00, Epsilon: 0.30, Rolling Avg:  27.60\n",
      "Config: DQN_16-32, Ep: 120/300, Reward:   40.00, Epsilon: 0.30, Rolling Avg:  28.65\n",
      "Config: DQN_16-32, Ep: 121/300, Reward:   35.00, Epsilon: 0.30, Rolling Avg:  29.60\n",
      "Config: DQN_16-32, Ep: 122/300, Reward:   63.00, Epsilon: 0.29, Rolling Avg:  31.90\n",
      "Config: DQN_16-32, Ep: 123/300, Reward:   46.00, Epsilon: 0.29, Rolling Avg:  33.35\n",
      "Config: DQN_16-32, Ep: 124/300, Reward:   49.00, Epsilon: 0.29, Rolling Avg:  34.95\n",
      "Config: DQN_16-32, Ep: 125/300, Reward:  112.00, Epsilon: 0.28, Rolling Avg:  38.90\n",
      "Config: DQN_16-32, Ep: 126/300, Reward:   39.00, Epsilon: 0.28, Rolling Avg:  39.65\n",
      "Config: DQN_16-32, Ep: 127/300, Reward:   48.00, Epsilon: 0.28, Rolling Avg:  40.95\n",
      "Config: DQN_16-32, Ep: 128/300, Reward:   21.00, Epsilon: 0.28, Rolling Avg:  41.05\n",
      "Config: DQN_16-32, Ep: 129/300, Reward:   22.00, Epsilon: 0.27, Rolling Avg:  40.25\n",
      "Config: DQN_16-32, Ep: 130/300, Reward:   17.00, Epsilon: 0.27, Rolling Avg:  39.45\n",
      "Config: DQN_16-32, Ep: 131/300, Reward:   22.00, Epsilon: 0.27, Rolling Avg:  39.50\n",
      "Config: DQN_16-32, Ep: 132/300, Reward:   15.00, Epsilon: 0.27, Rolling Avg:  38.95\n",
      "Config: DQN_16-32, Ep: 133/300, Reward:   29.00, Epsilon: 0.26, Rolling Avg:  38.95\n",
      "Config: DQN_16-32, Ep: 134/300, Reward:   12.00, Epsilon: 0.26, Rolling Avg:  38.15\n",
      "Config: DQN_16-32, Ep: 135/300, Reward:   14.00, Epsilon: 0.26, Rolling Avg:  36.75\n",
      "Config: DQN_16-32, Ep: 136/300, Reward:   12.00, Epsilon: 0.25, Rolling Avg:  34.30\n",
      "Config: DQN_16-32, Ep: 137/300, Reward:   24.00, Epsilon: 0.25, Rolling Avg:  33.25\n",
      "Config: DQN_16-32, Ep: 138/300, Reward:   24.00, Epsilon: 0.25, Rolling Avg:  33.45\n",
      "Config: DQN_16-32, Ep: 139/300, Reward:   32.00, Epsilon: 0.25, Rolling Avg:  33.80\n",
      "Config: DQN_16-32, Ep: 140/300, Reward:   27.00, Epsilon: 0.24, Rolling Avg:  33.15\n",
      "Config: DQN_16-32, Ep: 141/300, Reward:   37.00, Epsilon: 0.24, Rolling Avg:  33.25\n",
      "Config: DQN_16-32, Ep: 142/300, Reward:   25.00, Epsilon: 0.24, Rolling Avg:  31.35\n",
      "Config: DQN_16-32, Ep: 143/300, Reward:   40.00, Epsilon: 0.24, Rolling Avg:  31.05\n",
      "Config: DQN_16-32, Ep: 144/300, Reward:  118.00, Epsilon: 0.24, Rolling Avg:  34.50\n",
      "Config: DQN_16-32, Ep: 145/300, Reward:   91.00, Epsilon: 0.23, Rolling Avg:  33.45\n",
      "Config: DQN_16-32, Ep: 146/300, Reward:   67.00, Epsilon: 0.23, Rolling Avg:  34.85\n",
      "Config: DQN_16-32, Ep: 147/300, Reward:   33.00, Epsilon: 0.23, Rolling Avg:  34.10\n",
      "Config: DQN_16-32, Ep: 148/300, Reward:  105.00, Epsilon: 0.23, Rolling Avg:  38.30\n",
      "Config: DQN_16-32, Ep: 149/300, Reward:   99.00, Epsilon: 0.22, Rolling Avg:  42.15\n",
      "Config: DQN_16-32, Ep: 150/300, Reward:   85.00, Epsilon: 0.22, Rolling Avg:  45.55\n",
      "Config: DQN_16-32, Ep: 151/300, Reward:   76.00, Epsilon: 0.22, Rolling Avg:  48.25\n",
      "Config: DQN_16-32, Ep: 152/300, Reward:  187.00, Epsilon: 0.22, Rolling Avg:  56.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Find and print the best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 117\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    115\u001b[0m     DQN\u001b[38;5;241m.\u001b[39mfit(states, target_qs, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mexperience_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDQN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDQN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    120\u001b[0m     update_target_network(DQN, target_DQN)\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mexperience_replay\u001b[1;34m(batch_size, model)\u001b[0m\n\u001b[0;32m     57\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m sample_experiences(batch_size)\n\u001b[0;32m     58\u001b[0m target_qs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(states, verbose\u001b[38;5;241m=\u001b[39mverb)\n\u001b[1;32m---> 59\u001b[0m next_qs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m target_qs[np\u001b[38;5;241m.\u001b[39marange(batch_size), actions] \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(next_qs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones)\n\u001b[0;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(states, target_qs, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:499\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    497\u001b[0m ):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:720\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 720\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    722\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    723\u001b[0m         dataset\n\u001b[0;32m    724\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:235\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    233\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[1;32m--> 235\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n\u001b[0;32m    238\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    239\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:214\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[1;34m(indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mtraverse(grab_one, data)\n\u001b[1;32m--> 214\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization & deserialization\u001b[39;00m\n\u001b[0;32m    220\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2341\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:57\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`synchronous` is not supported with `num_parallel_calls`, but\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `num_parallel_calls` was set to \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m       num_parallel_calls,\n\u001b[0;32m     56\u001b[0m   )\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:202\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1256\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1255\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1257\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1226\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1230\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:303\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    299\u001b[0m placeholder_context \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mInternalPlaceholderContext(\n\u001b[0;32m    300\u001b[0m     func_graph, type_context\u001b[38;5;241m.\u001b[39mget_placeholder_mapping()\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m--> 303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplaceholder_context\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:356\u001b[0m, in \u001b[0;36mFunctionType.placeholder_arguments\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not generate placeholder value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartially defined function type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m   placeholder_context\u001b[38;5;241m.\u001b[39mupdate_naming_scope(parameter\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 356\u001b[0m   arguments[parameter\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplaceholder_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mBoundArguments(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:1021\u001b[0m, in \u001b[0;36mTensorSpec.placeholder_value\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_placeholder(context_graph, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1021\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_placeholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m   \u001b[38;5;66;03m# Record the requested/user-specified name in case it's different than\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m   \u001b[38;5;66;03m# the uniquified name, for validation when exporting signatures.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m   placeholder\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39m_set_attr(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_user_specified_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1028\u001b[0m       attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(s\u001b[38;5;241m=\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(name)))\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:1059\u001b[0m, in \u001b[0;36mTensorSpec._graph_placeholder\u001b[1;34m(self, graph, name)\u001b[0m\n\u001b[0;32m   1057\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape}\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1059\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1060\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlaceholder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1063\u001b[0m   \u001b[38;5;66;03m# TODO(b/262413656) Sometimes parameter names are not valid op names, in\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m   \u001b[38;5;66;03m# which case an unnamed placeholder is created instead. Update this logic\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m   \u001b[38;5;66;03m# to sanitize the name instead of falling back on unnamed placeholders.\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(e)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:614\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    612\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    613\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2705\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2705\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2706\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2711\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2712\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2713\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2714\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2715\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1200\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1200\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1057\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1054\u001b[0m                                          serialized)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1057\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Global Settings ===\n",
    "MAX_EPISODES = 300\n",
    "ROLLING_WINDOW = 20\n",
    "MAX_STEPS = 500\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "solved_threshold = 195\n",
    "verb = 0\n",
    "\n",
    "# === Replay Buffer (will be resized per experiment) ===\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "# === Model Builder ===\n",
    "def build_model(state_size, action_size, network_structure=(16, 32), learning_rate=0.001):\n",
    "    inputs = Input(shape=(state_size,))\n",
    "    x = inputs\n",
    "    for i, units in enumerate(network_structure):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    outputs = Dense(action_size, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# === Epsilon-Greedy Action Selection ===\n",
    "def select_action_greedy(state, model, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(model.output_shape[-1])\n",
    "    q_values = model.predict(state[np.newaxis, :], verbose=verb)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "# === Experience Replay Utilities ===\n",
    "def store(state, action, reward, next_state, done):\n",
    "    state = state.squeeze()\n",
    "    next_state = next_state.squeeze()\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "def experience_replay(batch_size, model):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    target_qs = model.predict(states, verbose=verb)\n",
    "    next_qs = model.predict(next_states, verbose=verb)\n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * np.max(next_qs, axis=1) * (1 - dones)\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "def update_target_network(source_model, target_model):\n",
    "    target_model.set_weights(source_model.get_weights())\n",
    "\n",
    "# === Experiment Config Class ===\n",
    "class ExperimentConfig:\n",
    "    def __init__(self, network_structure=(16, 32), memory_size=2000,\n",
    "                 learning_rate=0.001, epsilon_decay=0.99,\n",
    "                 algorithm=\"DQN\", name=\"default\"):\n",
    "        self.network_structure = network_structure\n",
    "        self.memory_size = memory_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.algorithm = algorithm\n",
    "        self.name = name\n",
    "\n",
    "# === Run a Single Experiment ===\n",
    "def run_experiment(config):\n",
    "    global replay_buffer\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    DQN = build_model(state_size, action_size, config.network_structure, config.learning_rate)\n",
    "    target_DQN = build_model(state_size, action_size, config.network_structure, config.learning_rate) if config.algorithm == \"DDQN\" else None\n",
    "    if target_DQN: update_target_network(DQN, target_DQN)\n",
    "    \n",
    "    replay_buffer = deque(maxlen=config.memory_size)\n",
    "    epsilon = 1.0\n",
    "    rewards_per_episode = []\n",
    "    rolling_avg_rewards = []\n",
    "    episodes_to_solve = None\n",
    "    model_filename = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            action = select_action_greedy(state, DQN, epsilon)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            store(state, action, reward, next_state, done or truncated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                if config.algorithm == \"DDQN\":\n",
    "                    states, actions, rewards_, next_states, dones = sample_experiences(batch_size)\n",
    "                    target_qs = DQN.predict(states, verbose=verb)\n",
    "                    next_actions = np.argmax(DQN.predict(next_states, verbose=verb), axis=1)\n",
    "                    next_qs = target_DQN.predict(next_states, verbose=verb)\n",
    "                    target_qs[np.arange(batch_size), actions] = rewards_ + gamma * next_qs[np.arange(batch_size), next_actions] * (1 - dones)\n",
    "                    DQN.fit(states, target_qs, epochs=1, verbose=0)\n",
    "                else:\n",
    "                    experience_replay(batch_size, DQN)\n",
    "\n",
    "                if config.algorithm == \"DDQN\" and episode % 10 == 0:\n",
    "                    update_target_network(DQN, target_DQN)\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * config.epsilon_decay)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "        rolling_avg_rewards.append(rolling_avg)\n",
    "\n",
    "        print(f\"Config: {config.name}, Ep: {episode+1:3}/{MAX_EPISODES}, \"\n",
    "              f\"Reward: {total_reward:7.2f}, Epsilon: {epsilon:.2f}, \"\n",
    "              f\"Rolling Avg: {rolling_avg:6.2f}\")\n",
    "\n",
    "        if rolling_avg >= solved_threshold and episodes_to_solve is None:\n",
    "            episodes_to_solve = episode + 1\n",
    "            model_filename = f\"CartPole_{config.algorithm}_{config.name}_solved.keras\"\n",
    "            DQN.save(model_filename)\n",
    "            print(f\"Model saved as {model_filename}\")\n",
    "            break\n",
    "\n",
    "    training_duration = (time.time() - start_time) / 60\n",
    "    if model_filename is None:\n",
    "        model_filename = f\"CartPole_{config.algorithm}_{config.name}_final.keras\"\n",
    "        DQN.save(model_filename)\n",
    "        print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return {\n",
    "        'config': config,\n",
    "        'episodes_to_solve': episodes_to_solve,\n",
    "        'training_duration': training_duration,\n",
    "        'rewards': rewards_per_episode,\n",
    "        'rolling_avg': rolling_avg_rewards,\n",
    "        'model_filename': model_filename\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_model(results):\n",
    "    # Filter only solved models\n",
    "    solved_models = [r for r in results if r['episodes_to_solve'] is not None]\n",
    "    \n",
    "    if not solved_models:\n",
    "        print(\"No models reached the solved threshold!\")\n",
    "        return None\n",
    "    \n",
    "    # Sort by episodes to solve (ascending) and training duration (ascending)\n",
    "    best_model = min(solved_models, \n",
    "                    key=lambda x: (x['episodes_to_solve'], x['training_duration']))\n",
    "    \n",
    "    print(\"\\nBest Model Found:\")\n",
    "    print(\"----------------\")\n",
    "    print(f\"Configuration: {best_model['config'].name}\")\n",
    "    print(f\"Algorithm: {best_model['config'].algorithm}\")\n",
    "    print(f\"Network Structure: {best_model['config'].network_structure}\")\n",
    "    print(f\"Memory Size: {best_model['config'].memory_size}\")\n",
    "    print(f\"Learning Rate: {best_model['config'].learning_rate}\")\n",
    "    print(f\"Epsilon Decay: {best_model['config'].epsilon_decay}\")\n",
    "    print(f\"Episodes to Solve: {best_model['episodes_to_solve']}\")\n",
    "    print(f\"Training Duration: {best_model['training_duration']:.2f} minutes\")\n",
    "    print(f\"Model File: {best_model['model_filename']}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Define experiments\n",
    "experiments = [\n",
    "    # Network structure comparison (both DQN and DDQN)\n",
    "    ExperimentConfig(network_structure=(16, 32), algorithm=\"DQN\", name=\"DQN_16-32\"),\n",
    "    ExperimentConfig(network_structure=(24, 24), algorithm=\"DQN\", name=\"DQN_24-24\"),\n",
    "    ExperimentConfig(network_structure=(32, 32), algorithm=\"DQN\", name=\"DQN_32-32\"),\n",
    "    ExperimentConfig(network_structure=(16, 32), algorithm=\"DDQN\", name=\"DDQN_16-32\"),\n",
    "    ExperimentConfig(network_structure=(24, 24), algorithm=\"DDQN\", name=\"DDQN_24-24\"),\n",
    "    ExperimentConfig(network_structure=(32, 32), algorithm=\"DDQN\", name=\"DDQN_32-32\"),\n",
    "\n",
    "    # Replay buffer comparison (DDQN only)\n",
    "    ExperimentConfig(memory_size=1000, algorithm=\"DDQN\", name=\"DDQN_mem_1000\"),\n",
    "    ExperimentConfig(memory_size=2000, algorithm=\"DDQN\", name=\"DDQN_mem_2000\"),\n",
    "    ExperimentConfig(memory_size=5000, algorithm=\"DDQN\", name=\"DDQN_mem_5000\"),\n",
    "\n",
    "    # Learning rate comparison (DDQN only)\n",
    "    ExperimentConfig(learning_rate=0.0001, algorithm=\"DDQN\", name=\"DDQN_lr_0.0001\"),\n",
    "    ExperimentConfig(learning_rate=0.001, algorithm=\"DDQN\", name=\"DDQN_lr_0.001\"),\n",
    "    ExperimentConfig(learning_rate=0.01, algorithm=\"DDQN\", name=\"DDQN_lr_0.01\"),\n",
    "\n",
    "    # Epsilon decay comparison (DDQN only)\n",
    "    ExperimentConfig(epsilon_decay=0.95, algorithm=\"DDQN\", name=\"DDQN_eps_0.95\"),\n",
    "    ExperimentConfig(epsilon_decay=0.99, algorithm=\"DDQN\", name=\"DDQN_eps_0.99\"),\n",
    "    ExperimentConfig(epsilon_decay=0.995, algorithm=\"DDQN\", name=\"DDQN_eps_0.995\"),\n",
    "]\n",
    "\n",
    "# Run experiments and collect results\n",
    "results = []\n",
    "for config in experiments:\n",
    "    print(f\"\\nRunning experiment: {config.name}\")\n",
    "    result = run_experiment(config)\n",
    "    results.append(result)\n",
    "\n",
    "# Find and print the best model\n",
    "best_model = find_best_model(results)\n",
    "\n",
    "# Analyze results\n",
    "def plot_experiment_results(results):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot rewards for each experiment\n",
    "    for result in results:\n",
    "        plt.plot(result['rolling_avg'], label=f\"{result['config'].name}\")\n",
    "    \n",
    "    plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "    plt.title('Comparison of Different Configurations')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rolling Average Reward')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nExperiment Results Summary:\")\n",
    "print(\"-------------------------\")\n",
    "for result in results:\n",
    "    config = result['config']\n",
    "    print(f\"\\nConfiguration: {config.name}\")\n",
    "    print(f\"Algorithm: {config.algorithm}\")\n",
    "    print(f\"Network Structure: {config.network_structure}\")\n",
    "    print(f\"Memory Size: {config.memory_size}\")\n",
    "    print(f\"Learning Rate: {config.learning_rate}\")\n",
    "    print(f\"Epsilon Decay: {config.epsilon_decay}\")\n",
    "    print(f\"Episodes to Solve: {result['episodes_to_solve']}\")\n",
    "    print(f\"Training Duration: {result['training_duration']:.2f} minutes\")\n",
    "    print(f\"Model File: {result['model_filename']}\")\n",
    "\n",
    "# Plot results\n",
    "plot_experiment_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c4e06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: DDQN_24-24\n",
      "Config: DDQN_24-24, Ep:   1/300, Reward:   16.00, Epsilon: 0.99, Rolling Avg:  16.00\n",
      "Config: DDQN_24-24, Ep:   2/300, Reward:   34.00, Epsilon: 0.98, Rolling Avg:  25.00\n",
      "Config: DDQN_24-24, Ep:   3/300, Reward:   25.00, Epsilon: 0.97, Rolling Avg:  25.00\n",
      "Config: DDQN_24-24, Ep:   4/300, Reward:    9.00, Epsilon: 0.96, Rolling Avg:  21.00\n",
      "Config: DDQN_24-24, Ep:   5/300, Reward:   30.00, Epsilon: 0.95, Rolling Avg:  22.80\n",
      "Config: DDQN_24-24, Ep:   6/300, Reward:    9.00, Epsilon: 0.94, Rolling Avg:  20.50\n",
      "Config: DDQN_24-24, Ep:   7/300, Reward:   15.00, Epsilon: 0.93, Rolling Avg:  19.71\n",
      "Config: DDQN_24-24, Ep:   8/300, Reward:   17.00, Epsilon: 0.92, Rolling Avg:  19.38\n",
      "Config: DDQN_24-24, Ep:   9/300, Reward:   60.00, Epsilon: 0.91, Rolling Avg:  23.89\n",
      "Config: DDQN_24-24, Ep:  10/300, Reward:   20.00, Epsilon: 0.90, Rolling Avg:  23.50\n",
      "Config: DDQN_24-24, Ep:  11/300, Reward:   28.00, Epsilon: 0.90, Rolling Avg:  23.91\n",
      "Config: DDQN_24-24, Ep:  12/300, Reward:   17.00, Epsilon: 0.89, Rolling Avg:  23.33\n",
      "Config: DDQN_24-24, Ep:  13/300, Reward:   25.00, Epsilon: 0.88, Rolling Avg:  23.46\n",
      "Config: DDQN_24-24, Ep:  14/300, Reward:   18.00, Epsilon: 0.87, Rolling Avg:  23.07\n",
      "Config: DDQN_24-24, Ep:  15/300, Reward:   21.00, Epsilon: 0.86, Rolling Avg:  22.93\n",
      "Config: DDQN_24-24, Ep:  16/300, Reward:   14.00, Epsilon: 0.85, Rolling Avg:  22.38\n",
      "Config: DDQN_24-24, Ep:  17/300, Reward:    9.00, Epsilon: 0.84, Rolling Avg:  21.59\n",
      "Config: DDQN_24-24, Ep:  18/300, Reward:   15.00, Epsilon: 0.83, Rolling Avg:  21.22\n",
      "Config: DDQN_24-24, Ep:  19/300, Reward:   27.00, Epsilon: 0.83, Rolling Avg:  21.53\n",
      "Config: DDQN_24-24, Ep:  20/300, Reward:   36.00, Epsilon: 0.82, Rolling Avg:  22.25\n",
      "Config: DDQN_24-24, Ep:  21/300, Reward:   12.00, Epsilon: 0.81, Rolling Avg:  22.05\n",
      "Config: DDQN_24-24, Ep:  22/300, Reward:   16.00, Epsilon: 0.80, Rolling Avg:  21.15\n",
      "Config: DDQN_24-24, Ep:  23/300, Reward:   17.00, Epsilon: 0.79, Rolling Avg:  20.75\n",
      "Config: DDQN_24-24, Ep:  24/300, Reward:   20.00, Epsilon: 0.79, Rolling Avg:  21.30\n",
      "Config: DDQN_24-24, Ep:  25/300, Reward:   32.00, Epsilon: 0.78, Rolling Avg:  21.40\n",
      "Config: DDQN_24-24, Ep:  26/300, Reward:   18.00, Epsilon: 0.77, Rolling Avg:  21.85\n",
      "Config: DDQN_24-24, Ep:  27/300, Reward:   41.00, Epsilon: 0.76, Rolling Avg:  23.15\n",
      "Config: DDQN_24-24, Ep:  28/300, Reward:   25.00, Epsilon: 0.75, Rolling Avg:  23.55\n",
      "Config: DDQN_24-24, Ep:  29/300, Reward:   35.00, Epsilon: 0.75, Rolling Avg:  22.30\n",
      "Config: DDQN_24-24, Ep:  30/300, Reward:   41.00, Epsilon: 0.74, Rolling Avg:  23.35\n",
      "Config: DDQN_24-24, Ep:  31/300, Reward:   19.00, Epsilon: 0.73, Rolling Avg:  22.90\n",
      "Config: DDQN_24-24, Ep:  32/300, Reward:   30.00, Epsilon: 0.72, Rolling Avg:  23.55\n",
      "Config: DDQN_24-24, Ep:  33/300, Reward:   31.00, Epsilon: 0.72, Rolling Avg:  23.85\n",
      "Config: DDQN_24-24, Ep:  34/300, Reward:   11.00, Epsilon: 0.71, Rolling Avg:  23.50\n",
      "Config: DDQN_24-24, Ep:  35/300, Reward:   57.00, Epsilon: 0.70, Rolling Avg:  25.30\n",
      "Config: DDQN_24-24, Ep:  36/300, Reward:   37.00, Epsilon: 0.70, Rolling Avg:  26.45\n",
      "Config: DDQN_24-24, Ep:  37/300, Reward:   31.00, Epsilon: 0.69, Rolling Avg:  27.55\n",
      "Config: DDQN_24-24, Ep:  38/300, Reward:   23.00, Epsilon: 0.68, Rolling Avg:  27.95\n",
      "Config: DDQN_24-24, Ep:  39/300, Reward:   56.00, Epsilon: 0.68, Rolling Avg:  29.40\n",
      "Config: DDQN_24-24, Ep:  40/300, Reward:   19.00, Epsilon: 0.67, Rolling Avg:  28.55\n",
      "Config: DDQN_24-24, Ep:  41/300, Reward:   36.00, Epsilon: 0.66, Rolling Avg:  29.75\n",
      "Config: DDQN_24-24, Ep:  42/300, Reward:   32.00, Epsilon: 0.66, Rolling Avg:  30.55\n",
      "Config: DDQN_24-24, Ep:  43/300, Reward:   67.00, Epsilon: 0.65, Rolling Avg:  33.05\n",
      "Config: DDQN_24-24, Ep:  44/300, Reward:   43.00, Epsilon: 0.64, Rolling Avg:  34.20\n",
      "Config: DDQN_24-24, Ep:  45/300, Reward:   51.00, Epsilon: 0.64, Rolling Avg:  35.15\n",
      "Config: DDQN_24-24, Ep:  46/300, Reward:   22.00, Epsilon: 0.63, Rolling Avg:  35.35\n",
      "Config: DDQN_24-24, Ep:  47/300, Reward:   53.00, Epsilon: 0.62, Rolling Avg:  35.95\n",
      "Config: DDQN_24-24, Ep:  48/300, Reward:   37.00, Epsilon: 0.62, Rolling Avg:  36.55\n",
      "Config: DDQN_24-24, Ep:  49/300, Reward:   12.00, Epsilon: 0.61, Rolling Avg:  35.40\n",
      "Config: DDQN_24-24, Ep:  50/300, Reward:   41.00, Epsilon: 0.61, Rolling Avg:  35.40\n",
      "Config: DDQN_24-24, Ep:  51/300, Reward:   19.00, Epsilon: 0.60, Rolling Avg:  35.40\n",
      "Config: DDQN_24-24, Ep:  52/300, Reward:   40.00, Epsilon: 0.59, Rolling Avg:  35.90\n",
      "Config: DDQN_24-24, Ep:  53/300, Reward:  135.00, Epsilon: 0.59, Rolling Avg:  41.10\n",
      "Config: DDQN_24-24, Ep:  54/300, Reward:   42.00, Epsilon: 0.58, Rolling Avg:  42.65\n",
      "Config: DDQN_24-24, Ep:  55/300, Reward:   12.00, Epsilon: 0.58, Rolling Avg:  40.40\n",
      "Config: DDQN_24-24, Ep:  56/300, Reward:   35.00, Epsilon: 0.57, Rolling Avg:  40.30\n",
      "Config: DDQN_24-24, Ep:  57/300, Reward:   45.00, Epsilon: 0.56, Rolling Avg:  41.00\n",
      "Config: DDQN_24-24, Ep:  58/300, Reward:   39.00, Epsilon: 0.56, Rolling Avg:  41.80\n",
      "Config: DDQN_24-24, Ep:  59/300, Reward:   66.00, Epsilon: 0.55, Rolling Avg:  42.30\n",
      "Config: DDQN_24-24, Ep:  60/300, Reward:   59.00, Epsilon: 0.55, Rolling Avg:  44.30\n",
      "Config: DDQN_24-24, Ep:  61/300, Reward:   20.00, Epsilon: 0.54, Rolling Avg:  43.50\n",
      "Config: DDQN_24-24, Ep:  62/300, Reward:  169.00, Epsilon: 0.54, Rolling Avg:  50.35\n",
      "Config: DDQN_24-24, Ep:  63/300, Reward:   85.00, Epsilon: 0.53, Rolling Avg:  51.25\n",
      "Config: DDQN_24-24, Ep:  64/300, Reward:   18.00, Epsilon: 0.53, Rolling Avg:  50.00\n",
      "Config: DDQN_24-24, Ep:  65/300, Reward:   68.00, Epsilon: 0.52, Rolling Avg:  50.85\n",
      "Config: DDQN_24-24, Ep:  66/300, Reward:   67.00, Epsilon: 0.52, Rolling Avg:  53.10\n",
      "Config: DDQN_24-24, Ep:  67/300, Reward:  118.00, Epsilon: 0.51, Rolling Avg:  56.35\n",
      "Config: DDQN_24-24, Ep:  68/300, Reward:   29.00, Epsilon: 0.50, Rolling Avg:  55.95\n",
      "Config: DDQN_24-24, Ep:  69/300, Reward:   25.00, Epsilon: 0.50, Rolling Avg:  56.60\n",
      "Config: DDQN_24-24, Ep:  70/300, Reward:  113.00, Epsilon: 0.49, Rolling Avg:  60.20\n",
      "Config: DDQN_24-24, Ep:  71/300, Reward:   83.00, Epsilon: 0.49, Rolling Avg:  63.40\n",
      "Config: DDQN_24-24, Ep:  72/300, Reward:  159.00, Epsilon: 0.48, Rolling Avg:  69.35\n",
      "Config: DDQN_24-24, Ep:  73/300, Reward:  209.00, Epsilon: 0.48, Rolling Avg:  73.05\n",
      "Config: DDQN_24-24, Ep:  74/300, Reward:  188.00, Epsilon: 0.48, Rolling Avg:  80.35\n",
      "Config: DDQN_24-24, Ep:  75/300, Reward:   78.00, Epsilon: 0.47, Rolling Avg:  83.65\n",
      "Config: DDQN_24-24, Ep:  76/300, Reward:  140.00, Epsilon: 0.47, Rolling Avg:  88.90\n",
      "Config: DDQN_24-24, Ep:  77/300, Reward:   88.00, Epsilon: 0.46, Rolling Avg:  91.05\n",
      "Config: DDQN_24-24, Ep:  78/300, Reward:   65.00, Epsilon: 0.46, Rolling Avg:  92.35\n",
      "Config: DDQN_24-24, Ep:  79/300, Reward:   66.00, Epsilon: 0.45, Rolling Avg:  92.35\n",
      "Config: DDQN_24-24, Ep:  80/300, Reward:   31.00, Epsilon: 0.45, Rolling Avg:  90.95\n",
      "Config: DDQN_24-24, Ep:  81/300, Reward:  183.00, Epsilon: 0.44, Rolling Avg:  99.10\n",
      "Config: DDQN_24-24, Ep:  82/300, Reward:  250.00, Epsilon: 0.44, Rolling Avg: 103.15\n",
      "Config: DDQN_24-24, Ep:  83/300, Reward:  119.00, Epsilon: 0.43, Rolling Avg: 104.85\n",
      "Config: DDQN_24-24, Ep:  84/300, Reward:   29.00, Epsilon: 0.43, Rolling Avg: 105.40\n",
      "Config: DDQN_24-24, Ep:  85/300, Reward:   76.00, Epsilon: 0.43, Rolling Avg: 105.80\n",
      "Config: DDQN_24-24, Ep:  86/300, Reward:  217.00, Epsilon: 0.42, Rolling Avg: 113.30\n",
      "Config: DDQN_24-24, Ep:  87/300, Reward:  153.00, Epsilon: 0.42, Rolling Avg: 115.05\n",
      "Config: DDQN_24-24, Ep:  88/300, Reward:   46.00, Epsilon: 0.41, Rolling Avg: 115.90\n",
      "Config: DDQN_24-24, Ep:  89/300, Reward:  102.00, Epsilon: 0.41, Rolling Avg: 119.75\n",
      "Config: DDQN_24-24, Ep:  90/300, Reward:  244.00, Epsilon: 0.40, Rolling Avg: 126.30\n",
      "Config: DDQN_24-24, Ep:  91/300, Reward:   80.00, Epsilon: 0.40, Rolling Avg: 126.15\n",
      "Config: DDQN_24-24, Ep:  92/300, Reward:  278.00, Epsilon: 0.40, Rolling Avg: 132.10\n",
      "Config: DDQN_24-24, Ep:  93/300, Reward:   15.00, Epsilon: 0.39, Rolling Avg: 122.40\n",
      "Config: DDQN_24-24, Ep:  94/300, Reward:  270.00, Epsilon: 0.39, Rolling Avg: 126.50\n",
      "Config: DDQN_24-24, Ep:  95/300, Reward:  191.00, Epsilon: 0.38, Rolling Avg: 132.15\n",
      "Config: DDQN_24-24, Ep:  96/300, Reward:  219.00, Epsilon: 0.38, Rolling Avg: 136.10\n",
      "Config: DDQN_24-24, Ep:  97/300, Reward:  231.00, Epsilon: 0.38, Rolling Avg: 143.25\n",
      "Config: DDQN_24-24, Ep:  98/300, Reward:   67.00, Epsilon: 0.37, Rolling Avg: 143.35\n",
      "Config: DDQN_24-24, Ep:  99/300, Reward:  142.00, Epsilon: 0.37, Rolling Avg: 147.15\n",
      "Config: DDQN_24-24, Ep: 100/300, Reward:  228.00, Epsilon: 0.37, Rolling Avg: 157.00\n",
      "Config: DDQN_24-24, Ep: 101/300, Reward:   53.00, Epsilon: 0.36, Rolling Avg: 150.50\n",
      "Config: DDQN_24-24, Ep: 102/300, Reward:  108.00, Epsilon: 0.36, Rolling Avg: 143.40\n",
      "Config: DDQN_24-24, Ep: 103/300, Reward:  268.00, Epsilon: 0.36, Rolling Avg: 150.85\n",
      "Config: DDQN_24-24, Ep: 104/300, Reward:  122.00, Epsilon: 0.35, Rolling Avg: 155.50\n",
      "Config: DDQN_24-24, Ep: 105/300, Reward:   69.00, Epsilon: 0.35, Rolling Avg: 155.15\n",
      "Config: DDQN_24-24, Ep: 106/300, Reward:  279.00, Epsilon: 0.34, Rolling Avg: 158.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 196\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 196\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# === Find Best Model ===\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 112\u001b[0m         \u001b[43mddqn_experience_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m TARGET_UPDATE_FREQ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    115\u001b[0m     update_target_network(main_model, target_model)\n",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m, in \u001b[0;36mddqn_experience_replay\u001b[1;34m(main_model, target_model)\u001b[0m\n\u001b[0;32m     60\u001b[0m next_qs \u001b[38;5;241m=\u001b[39m target_model\u001b[38;5;241m.\u001b[39mpredict(next_states, verbose\u001b[38;5;241m=\u001b[39mverb)\n\u001b[0;32m     61\u001b[0m target_qs[np\u001b[38;5;241m.\u001b[39marange(batch_size), actions] \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_qs[np\u001b[38;5;241m.\u001b[39marange(batch_size), next_actions] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones)\n\u001b[1;32m---> 62\u001b[0m \u001b[43mmain_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_qs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    367\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:102\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SABIO\\Documents\\GitHub\\IE-University\\myenv312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Settings ===\n",
    "MAX_EPISODES = 300\n",
    "ROLLING_WINDOW = 20\n",
    "MAX_STEPS = 500\n",
    "gamma = 0.99\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "solved_threshold = 195\n",
    "epsilon_start = 1.0\n",
    "verb = 0\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "\n",
    "# === Model Save Directory ===\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# === Build Model ===\n",
    "def build_model(state_size, action_size, network_structure=(64, 64), learning_rate=0.001):\n",
    "    inputs = Input(shape=(state_size,))\n",
    "    x = inputs\n",
    "    for units in network_structure:\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    outputs = Dense(action_size, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# === Replay Logic ===\n",
    "def select_action_greedy(state, model, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(model.output_shape[-1])\n",
    "    q_values = model.predict(state[np.newaxis, :], verbose=verb)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "def store(state, action, reward, next_state, done):\n",
    "    replay_buffer.append((state.squeeze(), action, reward, next_state.squeeze(), done))\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "def ddqn_experience_replay(main_model, target_model):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    target_qs = main_model.predict(states, verbose=verb)\n",
    "    next_actions = np.argmax(main_model.predict(next_states, verbose=verb), axis=1)\n",
    "    next_qs = target_model.predict(next_states, verbose=verb)\n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * next_qs[np.arange(batch_size), next_actions] * (1 - dones)\n",
    "    main_model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "def update_target_network(source_model, target_model):\n",
    "    target_model.set_weights(source_model.get_weights())\n",
    "\n",
    "# === Config Class ===\n",
    "class ExperimentConfig:\n",
    "    def __init__(self, network_structure=(64, 64), memory_size=2000,\n",
    "                 learning_rate=0.001, epsilon_decay=0.99, name=\"default\"):\n",
    "        self.network_structure = network_structure\n",
    "        self.memory_size = memory_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.name = name\n",
    "        self.algorithm = \"DDQN\"\n",
    "\n",
    "# === Train One Configuration ===\n",
    "def run_experiment(config):\n",
    "    global replay_buffer\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    main_model = build_model(state_size, action_size, config.network_structure, config.learning_rate)\n",
    "    target_model = build_model(state_size, action_size, config.network_structure, config.learning_rate)\n",
    "    update_target_network(main_model, target_model)\n",
    "\n",
    "    replay_buffer = deque(maxlen=config.memory_size)\n",
    "    epsilon = epsilon_start\n",
    "    rewards_per_episode = []\n",
    "    rolling_avg_rewards = []\n",
    "    episodes_to_solve = None\n",
    "    model_filename = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            action = select_action_greedy(state, main_model, epsilon)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            store(state, action, reward, next_state, done or truncated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                ddqn_experience_replay(main_model, target_model)\n",
    "\n",
    "        if episode % TARGET_UPDATE_FREQ == 0:\n",
    "            update_target_network(main_model, target_model)\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * config.epsilon_decay)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "        rolling_avg_rewards.append(rolling_avg)\n",
    "\n",
    "        print(f\"Config: {config.name}, Ep: {episode+1:3}/{MAX_EPISODES}, \"\n",
    "              f\"Reward: {total_reward:7.2f}, Epsilon: {epsilon:.2f}, \"\n",
    "              f\"Rolling Avg: {rolling_avg:6.2f}\")\n",
    "\n",
    "        if rolling_avg >= solved_threshold and episodes_to_solve is None:\n",
    "            episodes_to_solve = episode + 1\n",
    "            model_filename = os.path.join(MODEL_DIR, f\"CartPole_DDQN_{config.name}_solved.keras\")\n",
    "            main_model.save(model_filename)\n",
    "            print(f\"Model saved as {model_filename}\")\n",
    "            break\n",
    "\n",
    "    training_duration = (time.time() - start_time) / 60\n",
    "    if model_filename is None:\n",
    "        model_filename = os.path.join(MODEL_DIR, f\"CartPole_DDQN_{config.name}_final.keras\")\n",
    "        main_model.save(model_filename)\n",
    "        print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return {\n",
    "        'config': config,\n",
    "        'episodes_to_solve': episodes_to_solve,\n",
    "        'training_duration': training_duration,\n",
    "        'rewards': rewards_per_episode,\n",
    "        'rolling_avg': rolling_avg_rewards,\n",
    "        'model_filename': model_filename\n",
    "    }\n",
    "\n",
    "# === Best Model Finder ===\n",
    "def find_best_model(results):\n",
    "    solved_models = [r for r in results if r['episodes_to_solve'] is not None]\n",
    "    if not solved_models:\n",
    "        print(\"No models reached the solved threshold!\")\n",
    "        return None\n",
    "\n",
    "    best_model = min(solved_models, key=lambda x: (x['episodes_to_solve'], x['training_duration']))\n",
    "    print(\"\\nBest Model Found:\")\n",
    "    print(\"----------------\")\n",
    "    print(f\"Configuration: {best_model['config'].name}\")\n",
    "    print(f\"Algorithm: {best_model['config'].algorithm}\")\n",
    "    print(f\"Network Structure: {best_model['config'].network_structure}\")\n",
    "    print(f\"Memory Size: {best_model['config'].memory_size}\")\n",
    "    print(f\"Learning Rate: {best_model['config'].learning_rate}\")\n",
    "    print(f\"Epsilon Decay: {best_model['config'].epsilon_decay}\")\n",
    "    print(f\"Episodes to Solve: {best_model['episodes_to_solve']}\")\n",
    "    print(f\"Training Duration: {best_model['training_duration']:.2f} minutes\")\n",
    "    print(f\"Model File: {best_model['model_filename']}\")\n",
    "    return best_model\n",
    "\n",
    "# === Define Experiments ===\n",
    "experiments = [\n",
    "    # Network structure comparison\n",
    "    #ExperimentConfig(network_structure=(16, 32), name=\"DDQN_16-32\"),\n",
    "    ExperimentConfig(network_structure=(24, 24), name=\"DDQN_24-24\"),\n",
    "    #ExperimentConfig(network_structure=(32, 32), name=\"DDQN_32-32\"),\n",
    "\n",
    "    # Replay buffer comparison\n",
    "    ExperimentConfig(memory_size=1000, name=\"DDQN_mem_1000\"),\n",
    "    #ExperimentConfig(memory_size=2000, name=\"DDQN_mem_2000\"),\n",
    "    ExperimentConfig(memory_size=5000, name=\"DDQN_mem_5000\"),\n",
    "\n",
    "    # Learning rate comparison\n",
    "    ExperimentConfig(learning_rate=0.0001, name=\"DDQN_lr_0.0001\"),\n",
    "    #ExperimentConfig(learning_rate=0.001, name=\"DDQN_lr_0.001\"),\n",
    "    ExperimentConfig(learning_rate=0.01, name=\"DDQN_lr_0.01\"),\n",
    "\n",
    "    # Epsilon decay comparison\n",
    "    ExperimentConfig(epsilon_decay=0.95, name=\"DDQN_eps_0.95\"),\n",
    "    #ExperimentConfig(epsilon_decay=0.99, name=\"DDQN_eps_0.99\"),\n",
    "    ExperimentConfig(epsilon_decay=0.995, name=\"DDQN_eps_0.995\"),\n",
    "]\n",
    "\n",
    "# === Run All Experiments ===\n",
    "results = []\n",
    "for config in experiments:\n",
    "    print(f\"\\nRunning experiment: {config.name}\")\n",
    "    result = run_experiment(config)\n",
    "    results.append(result)\n",
    "\n",
    "# === Find Best Model ===\n",
    "best_model = find_best_model(results)\n",
    "\n",
    "# === Plot Results ===\n",
    "def plot_experiment_results(results):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for result in results:\n",
    "        plt.plot(result['rolling_avg'], label=f\"{result['config'].name}\")\n",
    "    plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "    plt.title('Comparison of Different Configurations')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rolling Average Reward')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# === Print Summary ===\n",
    "print(\"\\nExperiment Results Summary:\")\n",
    "print(\"-------------------------\")\n",
    "for result in results:\n",
    "    config = result['config']\n",
    "    print(f\"\\nConfiguration: {config.name}\")\n",
    "    print(f\"Network Structure: {config.network_structure}\")\n",
    "    print(f\"Memory Size: {config.memory_size}\")\n",
    "    print(f\"Learning Rate: {config.learning_rate}\")\n",
    "    print(f\"Epsilon Decay: {config.epsilon_decay}\")\n",
    "    print(f\"Episodes to Solve: {result['episodes_to_solve']}\")\n",
    "    print(f\"Training Duration: {result['training_duration']:.2f} minutes\")\n",
    "    print(f\"Model File: {result['model_filename']}\")\n",
    "\n",
    "plot_experiment_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
