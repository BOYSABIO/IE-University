{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 200000\n",
    "current_epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "decay_rate = 0.0001\n",
    "Reward_list = []\n",
    "\n",
    "max_steps = 100\n",
    "gamma = 0.99\n",
    "alpha = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(Q, env, cols, rows):\n",
    "    def action_to_symbol(action):\n",
    "        return ['↓', '↑', '→', '←', 'p', 'd'][action]\n",
    "\n",
    "    policy = np.zeros((rows, cols), dtype=str)\n",
    "    for state in range(env.observation_space.n):\n",
    "        if np.sum(Q[state] == 0):\n",
    "            policy[state // cols, state % cols] = 'o'\n",
    "        else:\n",
    "            best_action = np.argmax(Q[state])\n",
    "            policy[state // cols, state % cols] = action_to_symbol(best_action)\n",
    "    \n",
    "    desc = env.unwrapped.desc\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if desc[i][j] in b'|':\n",
    "                policy[i, j] = desc[i][j].decode('utf-8')\n",
    "    policy[0,0] = 'S'\n",
    "\n",
    "    print('LEARNED POLICY')\n",
    "    print()\n",
    "    for row in policy:\n",
    "        print(' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Average Reward (1000ep): -391.62\n",
      "Episode: 2000, Average Reward (1000ep): -389.46\n",
      "Episode: 3000, Average Reward (1000ep): -387.92\n",
      "Episode: 4000, Average Reward (1000ep): -389.66\n",
      "Episode: 5000, Average Reward (1000ep): -385.15\n",
      "Episode: 6000, Average Reward (1000ep): -388.55\n",
      "Episode: 7000, Average Reward (1000ep): -385.45\n",
      "Episode: 8000, Average Reward (1000ep): -387.34\n",
      "Episode: 9000, Average Reward (1000ep): -383.06\n",
      "Episode: 10000, Average Reward (1000ep): -381.17\n",
      "Episode: 11000, Average Reward (1000ep): -380.45\n",
      "Episode: 12000, Average Reward (1000ep): -380.25\n",
      "Episode: 13000, Average Reward (1000ep): -381.25\n",
      "Episode: 14000, Average Reward (1000ep): -378.51\n",
      "Episode: 15000, Average Reward (1000ep): -376.75\n",
      "Episode: 16000, Average Reward (1000ep): -374.76\n",
      "Episode: 17000, Average Reward (1000ep): -376.70\n",
      "Episode: 18000, Average Reward (1000ep): -376.01\n",
      "Episode: 19000, Average Reward (1000ep): -374.14\n",
      "Episode: 20000, Average Reward (1000ep): -372.00\n",
      "Episode: 21000, Average Reward (1000ep): -368.57\n",
      "Episode: 22000, Average Reward (1000ep): -369.11\n",
      "Episode: 23000, Average Reward (1000ep): -370.26\n",
      "Episode: 24000, Average Reward (1000ep): -363.21\n",
      "Episode: 25000, Average Reward (1000ep): -363.22\n",
      "Episode: 26000, Average Reward (1000ep): -366.47\n",
      "Episode: 27000, Average Reward (1000ep): -363.18\n",
      "Episode: 28000, Average Reward (1000ep): -356.44\n",
      "Episode: 29000, Average Reward (1000ep): -359.26\n",
      "Episode: 30000, Average Reward (1000ep): -356.55\n",
      "Episode: 31000, Average Reward (1000ep): -355.55\n",
      "Episode: 32000, Average Reward (1000ep): -346.61\n",
      "Episode: 33000, Average Reward (1000ep): -350.48\n",
      "Episode: 34000, Average Reward (1000ep): -350.14\n",
      "Episode: 35000, Average Reward (1000ep): -348.29\n",
      "Episode: 36000, Average Reward (1000ep): -341.52\n",
      "Episode: 37000, Average Reward (1000ep): -343.39\n",
      "Episode: 38000, Average Reward (1000ep): -338.66\n",
      "Episode: 39000, Average Reward (1000ep): -337.67\n",
      "Episode: 40000, Average Reward (1000ep): -328.21\n",
      "Episode: 41000, Average Reward (1000ep): -334.97\n",
      "Episode: 42000, Average Reward (1000ep): -326.64\n",
      "Episode: 43000, Average Reward (1000ep): -333.48\n",
      "Episode: 44000, Average Reward (1000ep): -317.14\n",
      "Episode: 45000, Average Reward (1000ep): -321.60\n",
      "Episode: 46000, Average Reward (1000ep): -318.59\n",
      "Episode: 47000, Average Reward (1000ep): -323.83\n",
      "Episode: 48000, Average Reward (1000ep): -305.49\n",
      "Episode: 49000, Average Reward (1000ep): -304.39\n",
      "Episode: 50000, Average Reward (1000ep): -297.44\n",
      "Episode: 51000, Average Reward (1000ep): -308.91\n",
      "Episode: 52000, Average Reward (1000ep): -297.50\n",
      "Episode: 53000, Average Reward (1000ep): -292.95\n",
      "Episode: 54000, Average Reward (1000ep): -270.63\n",
      "Episode: 55000, Average Reward (1000ep): -266.95\n",
      "Episode: 56000, Average Reward (1000ep): -269.43\n",
      "Episode: 57000, Average Reward (1000ep): -295.13\n",
      "Episode: 58000, Average Reward (1000ep): -265.23\n",
      "Episode: 59000, Average Reward (1000ep): -281.60\n",
      "Episode: 60000, Average Reward (1000ep): -280.21\n",
      "Episode: 61000, Average Reward (1000ep): -256.51\n",
      "Episode: 62000, Average Reward (1000ep): -256.07\n",
      "Episode: 63000, Average Reward (1000ep): -264.16\n",
      "Episode: 64000, Average Reward (1000ep): -216.91\n",
      "Episode: 65000, Average Reward (1000ep): -259.13\n",
      "Episode: 66000, Average Reward (1000ep): -245.14\n",
      "Episode: 67000, Average Reward (1000ep): -252.06\n",
      "Episode: 68000, Average Reward (1000ep): -220.30\n",
      "Episode: 69000, Average Reward (1000ep): -209.22\n",
      "Episode: 70000, Average Reward (1000ep): -234.31\n",
      "Episode: 71000, Average Reward (1000ep): -221.96\n",
      "Episode: 72000, Average Reward (1000ep): -213.37\n",
      "Episode: 73000, Average Reward (1000ep): -221.76\n",
      "Episode: 74000, Average Reward (1000ep): -243.99\n",
      "Episode: 75000, Average Reward (1000ep): -204.27\n",
      "Episode: 76000, Average Reward (1000ep): -206.97\n",
      "Episode: 77000, Average Reward (1000ep): -201.67\n",
      "Episode: 78000, Average Reward (1000ep): -172.18\n",
      "Episode: 79000, Average Reward (1000ep): -225.75\n",
      "Episode: 80000, Average Reward (1000ep): -171.91\n",
      "Episode: 81000, Average Reward (1000ep): -203.40\n",
      "Episode: 82000, Average Reward (1000ep): -169.63\n",
      "Episode: 83000, Average Reward (1000ep): -162.63\n",
      "Episode: 84000, Average Reward (1000ep): -182.03\n",
      "Episode: 85000, Average Reward (1000ep): -172.89\n",
      "Episode: 86000, Average Reward (1000ep): -193.63\n",
      "Episode: 87000, Average Reward (1000ep): -184.69\n",
      "Episode: 88000, Average Reward (1000ep): -164.26\n",
      "Episode: 89000, Average Reward (1000ep): -174.39\n",
      "Episode: 90000, Average Reward (1000ep): -172.25\n",
      "Episode: 91000, Average Reward (1000ep): -132.90\n",
      "Episode: 92000, Average Reward (1000ep): -158.78\n",
      "Episode: 93000, Average Reward (1000ep): -162.71\n",
      "Episode: 94000, Average Reward (1000ep): -127.45\n",
      "Episode: 95000, Average Reward (1000ep): -172.50\n",
      "Episode: 96000, Average Reward (1000ep): -150.03\n",
      "Episode: 97000, Average Reward (1000ep): -207.82\n",
      "Episode: 98000, Average Reward (1000ep): -125.36\n",
      "Episode: 99000, Average Reward (1000ep): -133.92\n",
      "Episode: 100000, Average Reward (1000ep): -146.99\n",
      "Episode: 101000, Average Reward (1000ep): -153.06\n",
      "Episode: 102000, Average Reward (1000ep): -138.24\n",
      "Episode: 103000, Average Reward (1000ep): -177.41\n",
      "Episode: 104000, Average Reward (1000ep): -113.99\n",
      "Episode: 105000, Average Reward (1000ep): -86.26\n",
      "Episode: 106000, Average Reward (1000ep): -186.56\n",
      "Episode: 107000, Average Reward (1000ep): -136.53\n",
      "Episode: 108000, Average Reward (1000ep): -137.46\n",
      "Episode: 109000, Average Reward (1000ep): -142.01\n",
      "Episode: 110000, Average Reward (1000ep): -131.97\n",
      "Episode: 111000, Average Reward (1000ep): -105.54\n",
      "Episode: 112000, Average Reward (1000ep): -113.31\n",
      "Episode: 113000, Average Reward (1000ep): -159.47\n",
      "Episode: 114000, Average Reward (1000ep): -82.99\n",
      "Episode: 115000, Average Reward (1000ep): -146.38\n",
      "Episode: 116000, Average Reward (1000ep): -140.36\n",
      "Episode: 117000, Average Reward (1000ep): -157.48\n",
      "Episode: 118000, Average Reward (1000ep): -128.72\n",
      "Episode: 119000, Average Reward (1000ep): -108.31\n",
      "Episode: 120000, Average Reward (1000ep): -129.58\n",
      "Episode: 121000, Average Reward (1000ep): -106.31\n",
      "Episode: 122000, Average Reward (1000ep): -93.69\n",
      "Episode: 123000, Average Reward (1000ep): -84.72\n",
      "Episode: 124000, Average Reward (1000ep): -140.56\n",
      "Episode: 125000, Average Reward (1000ep): -130.25\n",
      "Episode: 126000, Average Reward (1000ep): -82.23\n",
      "Episode: 127000, Average Reward (1000ep): -102.88\n",
      "Episode: 128000, Average Reward (1000ep): -153.80\n",
      "Episode: 129000, Average Reward (1000ep): -168.26\n",
      "Episode: 130000, Average Reward (1000ep): -138.83\n",
      "Episode: 131000, Average Reward (1000ep): -100.47\n",
      "Episode: 132000, Average Reward (1000ep): -45.86\n",
      "Episode: 133000, Average Reward (1000ep): -56.00\n",
      "Episode: 134000, Average Reward (1000ep): -64.37\n",
      "Episode: 135000, Average Reward (1000ep): -153.18\n",
      "Episode: 136000, Average Reward (1000ep): -130.18\n",
      "Episode: 137000, Average Reward (1000ep): -63.70\n",
      "Episode: 138000, Average Reward (1000ep): -116.86\n",
      "Episode: 139000, Average Reward (1000ep): -83.50\n",
      "Episode: 140000, Average Reward (1000ep): -71.22\n",
      "Episode: 141000, Average Reward (1000ep): -93.30\n",
      "Episode: 142000, Average Reward (1000ep): -115.86\n",
      "Episode: 143000, Average Reward (1000ep): -114.28\n",
      "Episode: 144000, Average Reward (1000ep): -86.93\n",
      "Episode: 145000, Average Reward (1000ep): -82.33\n",
      "Episode: 146000, Average Reward (1000ep): -121.59\n",
      "Episode: 147000, Average Reward (1000ep): -112.46\n",
      "Episode: 148000, Average Reward (1000ep): -108.59\n",
      "Episode: 149000, Average Reward (1000ep): -93.44\n",
      "Episode: 150000, Average Reward (1000ep): -103.80\n",
      "Episode: 151000, Average Reward (1000ep): -120.98\n",
      "Episode: 152000, Average Reward (1000ep): -116.97\n",
      "Episode: 153000, Average Reward (1000ep): -126.14\n",
      "Episode: 154000, Average Reward (1000ep): -75.16\n",
      "Episode: 155000, Average Reward (1000ep): -75.46\n",
      "Episode: 156000, Average Reward (1000ep): -70.61\n",
      "Episode: 157000, Average Reward (1000ep): -113.95\n",
      "Episode: 158000, Average Reward (1000ep): -159.28\n",
      "Episode: 159000, Average Reward (1000ep): -128.00\n",
      "Episode: 160000, Average Reward (1000ep): -145.18\n",
      "Episode: 161000, Average Reward (1000ep): -98.05\n",
      "Episode: 162000, Average Reward (1000ep): -65.14\n",
      "Episode: 163000, Average Reward (1000ep): -68.72\n",
      "Episode: 164000, Average Reward (1000ep): -50.15\n",
      "Episode: 165000, Average Reward (1000ep): -56.55\n",
      "Episode: 166000, Average Reward (1000ep): -75.74\n",
      "Episode: 167000, Average Reward (1000ep): -57.12\n",
      "Episode: 168000, Average Reward (1000ep): -122.82\n",
      "Episode: 169000, Average Reward (1000ep): -152.31\n",
      "Episode: 170000, Average Reward (1000ep): -95.50\n",
      "Episode: 171000, Average Reward (1000ep): -78.50\n",
      "Episode: 172000, Average Reward (1000ep): -72.32\n",
      "Episode: 173000, Average Reward (1000ep): -71.52\n",
      "Episode: 174000, Average Reward (1000ep): -67.35\n",
      "Episode: 175000, Average Reward (1000ep): -48.30\n",
      "Episode: 176000, Average Reward (1000ep): -74.65\n",
      "Episode: 177000, Average Reward (1000ep): -151.66\n",
      "Episode: 178000, Average Reward (1000ep): -155.40\n",
      "Episode: 179000, Average Reward (1000ep): -150.22\n",
      "Episode: 180000, Average Reward (1000ep): -109.97\n",
      "Episode: 181000, Average Reward (1000ep): -98.42\n",
      "Episode: 182000, Average Reward (1000ep): -97.24\n",
      "Episode: 183000, Average Reward (1000ep): -81.86\n",
      "Episode: 184000, Average Reward (1000ep): -75.37\n",
      "Episode: 185000, Average Reward (1000ep): -58.68\n",
      "Episode: 186000, Average Reward (1000ep): -53.95\n",
      "Episode: 187000, Average Reward (1000ep): -47.59\n",
      "Episode: 188000, Average Reward (1000ep): -32.46\n",
      "Episode: 189000, Average Reward (1000ep): -27.05\n",
      "Episode: 190000, Average Reward (1000ep): -28.46\n",
      "Episode: 191000, Average Reward (1000ep): -60.91\n",
      "Episode: 192000, Average Reward (1000ep): -55.82\n",
      "Episode: 193000, Average Reward (1000ep): -47.82\n",
      "Episode: 194000, Average Reward (1000ep): -47.58\n",
      "Episode: 195000, Average Reward (1000ep): -42.87\n",
      "Episode: 196000, Average Reward (1000ep): -34.94\n",
      "Episode: 197000, Average Reward (1000ep): -34.27\n",
      "Episode: 198000, Average Reward (1000ep): -28.73\n",
      "Episode: 199000, Average Reward (1000ep): -31.16\n",
      "Episode: 200000, Average Reward (1000ep): -31.68\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    epsilon = max(epsilon_min, epsilon_max - (epsilon_max - epsilon_min) * (episode / n_episodes))\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_states.append(state)\n",
    "        episode_actions.append(action)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    G = 0\n",
    "    for t in range(len(episode_states) -1, -1, -1):\n",
    "        state = episode_states[t]\n",
    "        action = episode_actions[t]\n",
    "        G = gamma * G + episode_rewards[t]\n",
    "\n",
    "        N[state, action] += 1\n",
    "        Q[state, action] += (alpha * (G - Q[state, action]))\n",
    "    \n",
    "    total_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        avg_reward = np.mean(total_rewards[-1000:])\n",
    "        print(f\"Episode: {episode + 1}, Average Reward (1000ep): {avg_reward:.2f}\")\n",
    "\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 test episodes: -35.87\n"
     ]
    }
   ],
   "source": [
    "# Test the learned policy\n",
    "n_test_episodes = 100\n",
    "test_rewards = []\n",
    "\n",
    "for _ in range(n_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = np.argmax(Q[state])\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward over {n_test_episodes} test episodes: {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample policy (first 100 states):\n",
      "State 0: ↓ (South (↓))\n",
      "State 1: p (Pickup (p))\n",
      "State 2: p (Pickup (p))\n",
      "State 3: ↓ (South (↓))\n",
      "State 4: ↓ (South (↓))\n",
      "State 5: ↓ (South (↓))\n",
      "State 6: ↓ (South (↓))\n",
      "State 7: → (East (→))\n",
      "State 8: ↓ (South (↓))\n",
      "State 9: → (East (→))\n",
      "State 10: ↓ (South (↓))\n",
      "State 11: ↓ (South (↓))\n",
      "State 12: → (East (→))\n",
      "State 13: ↓ (South (↓))\n",
      "State 14: ↓ (South (↓))\n",
      "State 15: ↓ (South (↓))\n",
      "State 16: d (Dropoff (d))\n",
      "State 17: ↓ (South (↓))\n",
      "State 18: ↓ (South (↓))\n",
      "State 19: p (Pickup (p))\n",
      "State 20: ↓ (South (↓))\n",
      "State 21: ← (West (←))\n",
      "State 22: ↓ (South (↓))\n",
      "State 23: ← (West (←))\n",
      "State 24: ↓ (South (↓))\n",
      "State 25: ↓ (South (↓))\n",
      "State 26: ← (West (←))\n",
      "State 27: ↓ (South (↓))\n",
      "State 28: ← (West (←))\n",
      "State 29: ↓ (South (↓))\n",
      "State 30: ↓ (South (↓))\n",
      "State 31: ← (West (←))\n",
      "State 32: ↓ (South (↓))\n",
      "State 33: ↓ (South (↓))\n",
      "State 34: ↓ (South (↓))\n",
      "State 35: ↓ (South (↓))\n",
      "State 36: ← (West (←))\n",
      "State 37: ← (West (←))\n",
      "State 38: ← (West (←))\n",
      "State 39: p (Pickup (p))\n",
      "State 40: ↓ (South (↓))\n",
      "State 41: ↓ (South (↓))\n",
      "State 42: ↓ (South (↓))\n",
      "State 43: ↑ (North (↑))\n",
      "State 44: → (East (→))\n",
      "State 45: ↓ (South (↓))\n",
      "State 46: → (East (→))\n",
      "State 47: → (East (→))\n",
      "State 48: → (East (→))\n",
      "State 49: ↓ (South (↓))\n",
      "State 50: ↓ (South (↓))\n",
      "State 51: ↓ (South (↓))\n",
      "State 52: ↓ (South (↓))\n",
      "State 53: → (East (→))\n",
      "State 54: ← (West (←))\n",
      "State 55: ↓ (South (↓))\n",
      "State 56: → (East (→))\n",
      "State 57: → (East (→))\n",
      "State 58: → (East (→))\n",
      "State 59: → (East (→))\n",
      "State 60: ↓ (South (↓))\n",
      "State 61: ↓ (South (↓))\n",
      "State 62: ↓ (South (↓))\n",
      "State 63: ↑ (North (↑))\n",
      "State 64: ↓ (South (↓))\n",
      "State 65: ↓ (South (↓))\n",
      "State 66: ↓ (South (↓))\n",
      "State 67: → (East (→))\n",
      "State 68: ↓ (South (↓))\n",
      "State 69: ← (West (←))\n",
      "State 70: ↓ (South (↓))\n",
      "State 71: ← (West (←))\n",
      "State 72: → (East (→))\n",
      "State 73: ↓ (South (↓))\n",
      "State 74: ↓ (South (↓))\n",
      "State 75: ↓ (South (↓))\n",
      "State 76: ↓ (South (↓))\n",
      "State 77: → (East (→))\n",
      "State 78: ↓ (South (↓))\n",
      "State 79: ↓ (South (↓))\n",
      "State 80: ↓ (South (↓))\n",
      "State 81: ↓ (South (↓))\n",
      "State 82: ← (West (←))\n",
      "State 83: ↓ (South (↓))\n",
      "State 84: p (Pickup (p))\n",
      "State 85: ↓ (South (↓))\n",
      "State 86: p (Pickup (p))\n",
      "State 87: p (Pickup (p))\n",
      "State 88: ← (West (←))\n",
      "State 89: ↓ (South (↓))\n",
      "State 90: ↓ (South (↓))\n",
      "State 91: ← (West (←))\n",
      "State 92: ↓ (South (↓))\n",
      "State 93: ↓ (South (↓))\n",
      "State 94: ← (West (←))\n",
      "State 95: ↓ (South (↓))\n",
      "State 96: ↓ (South (↓))\n",
      "State 97: d (Dropoff (d))\n",
      "State 98: ← (West (←))\n",
      "State 99: ↓ (South (↓))\n"
     ]
    }
   ],
   "source": [
    "def action_to_symbol(action):\n",
    "    return ['↓', '↑', '→', '←', 'p', 'd'][action]\n",
    "\n",
    "def summarize_policy(Q, env, n=10):\n",
    "    print(\"Sample policy (first {} states):\".format(n))\n",
    "    ACTION_MEANINGS = ['South (↓)', 'North (↑)', 'East (→)', 'West (←)', 'Pickup (p)', 'Dropoff (d)']\n",
    "    for state in range(n):\n",
    "        best_action = np.argmax(Q[state])\n",
    "        symbol = action_to_symbol(best_action)\n",
    "        print(f\"State {state}: {symbol} ({ACTION_MEANINGS[best_action]})\")\n",
    "\n",
    "summarize_policy(Q, env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 3 test episodes: 8.00\n"
     ]
    }
   ],
   "source": [
    "# Simulate one episode with the learned policy\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "n_test_episodes = 3\n",
    "test_rewards = []\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for _ in range(n_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = np.argmax(Q[state])\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    test_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward over {n_test_episodes} test episodes: {np.mean(test_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprint_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m, in \u001b[0;36mprint_policy\u001b[1;34m(Q, env, cols, rows)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         best_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[state])\n\u001b[1;32m---> 11\u001b[0m         \u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m action_to_symbol(best_action)\n\u001b[0;32m     13\u001b[0m desc \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mdesc\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rows):\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "source": [
    "print_policy(Q,env, 11, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
