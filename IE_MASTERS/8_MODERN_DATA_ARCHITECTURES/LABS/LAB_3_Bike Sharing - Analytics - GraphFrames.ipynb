{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"https://storage.googleapis.com/kaggle-datasets-images/57/116/08a7f99f23e148898ab0eda150afc99f/dataset-cover.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Lab](#2)\n",
    "  * [2.1 Check Lab Files](#2.1)\n",
    "  * [2.2 Create the DataFrames](#2.2)\n",
    "  * [2.3 Create the GraphFrame](#2.3)\n",
    "  * [2.4 Analytics](#2.3)\n",
    "* [3. TearDown](#3)\n",
    "  * [3.1 Stop Hadoop](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goal for this lab are:</div>\n",
    "<ul>    \n",
    "    <li>Get familiar with Spark GraphFrames API</li>\n",
    "</ul>    \n",
    "</p>\n",
    "\n",
    "<p>We are going to work with a bike sharing <a href=\"https://www.kaggle.com/benhamner/sf-bay-area-bike-share\">dataset</a></p>\n",
    "<p>Actually a smaller version with two files: 201508_trip_data.csv & 201508_station_data.csv</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop\n",
    "\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-start.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster.<br/>\n",
    "GraphFrames is not in spark core so we have to add it this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\" --jars /opt/hive3/lib/hive-hcatalog-core-3.1.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Bike Sharing - Analytics - GraphFrames\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Check Lab Files\n",
    "\n",
    "In order to complete this lab you need to previosly upload the datasets into HDFS.<br/>\n",
    "\n",
    "Check you have the data ready in HDFS\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/raw/san-francisco-bay-bike-sharing/stations/\n",
    "\n",
    "http://localhost:50070/explorer.html#/datalake/raw/san-francisco-bay-bike-sharing/trips/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Create the DataFrames\n",
    "\n",
    "The first step after creating the SparkSession is to create one or more DataFrames<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = (spark.read\n",
    "            .option(\"header\",\"true\")\n",
    "            .option(\"inferSchema\",\"true\")\n",
    "            .csv(\"hdfs://localhost:9000/datalake/raw/san-francisco-bay-bike-sharing/stations/\")            \n",
    "            .distinct())\n",
    "\n",
    "trips = (spark.read\n",
    "                  .option(\"header\",\"true\")\n",
    "                  .option(\"inferSchema\",\"true\") \n",
    "                  .csv(\"hdfs://localhost:9000/datalake/raw/san-francisco-bay-bike-sharing/trips/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is related to different areas in San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.select(\"landmark\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.3'></a>\n",
    "### 2.3 Create the GraphFrame\n",
    "\n",
    "We are going to model our graph in the following way:<br/>\n",
    "**vertices** : stations <br/>\n",
    "**edges** : trips aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,avg,desc,asc,col\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# GraphFrames requires the vertices DataFrame to have a column named id.\n",
    "vertices = stations.withColumnRenamed(\"station_id\",\"id\")\n",
    "    \n",
    "# GraphFrames requires the edges DataFrame to have columns named src and dst\n",
    "trips = (trips.withColumnRenamed(\"Start Terminal\", \"src\")\n",
    "              .withColumnRenamed(\"End Terminal\", \"dst\")\n",
    "              .withColumnRenamed(\"Start Station\", \"src_name\")\n",
    "              .withColumnRenamed(\"End Station\", \"dst_name\"))\n",
    "              \n",
    "edges = (trips.groupBy(\"src\",\"src_name\", \"dst\", \"dst_name\")\n",
    "              .agg(\n",
    "                  count(\"*\").alias(\"trip_count\"),\n",
    "                  avg(\"duration\").alias(\"duration_avg\")\n",
    "              ))\n",
    "                            \n",
    "     \n",
    "# Creates the graph\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# graph processing requires recursive/iterative calculations so is a good practice to cache\n",
    "graph.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.vertices.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a subgraph for the sake of practicing with the stations related to \"San Francisco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = GraphFrame(graph.vertices.where(\"landmark='San Francisco'\"),graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.4'></a>\n",
    "### 2.4 Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### which are the top 5 most common routes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = subgraph.edges.orderBy(desc(\"trip_count\"))\n",
    "\n",
    "routes.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which are the stations where most of the trips depart from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDeg = subgraph.inDegrees\n",
    "inDeg.orderBy(desc(\"inDegree\"),asc(\"id\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDeg.join(subgraph.vertices,\"id\").orderBy(desc(\"inDegree\"),asc(\"id\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which are the stations where most of the trips get to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDeg = subgraph.outDegrees\n",
    "outDeg.orderBy(desc(\"outDegree\"),asc(\"id\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDeg.join(subgraph.vertices,\"id\").orderBy(desc(\"outDegree\"),asc(\"id\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### which are the most relevant stations?\n",
    "We are going to apply the Page Ranks algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = subgraph.pageRank(resetProbability=0.15, maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm returns a GraphFrame. <br/>\n",
    "Notice we now have a new column in the vertices DataFrame called **pagerank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.vertices.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we now have a new column in the edges DataFrame called **weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.edges.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop Hadoop\n",
    "\n",
    "Stops Hadoop\n",
    "Open a terminal and execute\n",
    "```sh\n",
    "hadoop-stop.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
