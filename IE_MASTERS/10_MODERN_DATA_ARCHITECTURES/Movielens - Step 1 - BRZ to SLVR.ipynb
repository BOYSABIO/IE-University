{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1)\n",
    "  * [1.1 Start Hadoop](#1.1)  \n",
    "  * [1.2 Set Global Config](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Lab](#2)\n",
    "  * [2.1 Check Files](#2.1)\n",
    "  * [2.2 Read Bronze DataFrames](#2.3)\n",
    "  * [2.3 Transform Bronze DataFrames](#2.3)\n",
    "  * [2.4 Write DataFrames to Silver](#2.4)\n",
    "  * [2.5 All at once](#2.5)\n",
    "* [3. TearDown](#3)\n",
    "  * [3.1 Stop Hadoop](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>The goals for this lab are:</div>\n",
    "<ul>    \n",
    "    <li>Get familiar with Spark DataFrames API</li>\n",
    "    <li>Apply some transformations using Spark DataFrames API</li>\n",
    "    <li>Promote data from bronze to silver layer in the datalake using Spark DataFrames API</li>\n",
    "</ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start Hadoop\n",
    "\n",
    "Start Hadoop <a href=\"http://localhost:2024/\">here </a>\n",
    "\n",
    "<p>\n",
    "<img style=\"width:48px\" src=\"https://cdn.iconscout.com/icon/free/png-256/free-hadoop-226007.png\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Set Global Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm changing pandas max column width property to improve data displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current notebook name\n",
    "notebook_name = __session__.replace('.ipynb','')[__session__.rfind('/')+1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS base paths\n",
    "hdfs_lakehouse_base_path = 'hdfs://localhost:9000/lakehouse/'\n",
    "hdfs_warehouse_base_path = 'hdfs://localhost:9000/warehouse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies = [\"org.apache.spark:spark-avro_2.12:3.5.0\",\n",
    "                \"io.delta:delta-iceberg_2.12:3.0.0\"]\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']= f\"--packages {','.join(dependencies)} pyspark-shell\"\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(notebook_name)\n",
    "    .config(\"spark.log.level\",\"ERROR\")\n",
    "    .config(\"spark.sql.warehouse.dir\",hdfs_warehouse_base_path)\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Check  Files\n",
    "\n",
    "Check you have the data ready in HDFS\n",
    "\n",
    "http://localhost:50070/explorer.html#/lakehouse/bronze/movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Read Bronze DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_brz = (spark.read\n",
    "              .option(\"header\",\"true\")\n",
    "              .option(\"escape\",\"\\\"\")\n",
    "              .csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/movies/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_brz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_brz.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_brz = spark.read.option(\"header\",\"true\").csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/ratings/\")\n",
    "ratings_brz.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_brz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_brz = spark.read.option(\"header\",\"true\").csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/links/\")\n",
    "links_brz.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_brz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailers_brz = spark.read.option(\"header\",\"true\").csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/trailers/\")\n",
    "trailers_brz.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailers_brz.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.3'></a>\n",
    "### 2.3 Transform Bronze DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "@udf(\"array<string>\")\n",
    "def parse_title(t:str):\n",
    "    import re\n",
    "    titleRegex = re.compile(r'^(.+)\\((\\d{4})\\)$')\n",
    "    m = titleRegex.search(t.strip())\n",
    "    if m:\n",
    "        title,year= m.groups()\n",
    "        return [title.strip(),year.strip()]\n",
    "    else:\n",
    "        return [t,None]\n",
    "    \n",
    "\n",
    "\n",
    "movies_slv = movies_brz.select(\n",
    "    col(\"movieId\").cast(\"bigint\"),\n",
    "    parse_title(col(\"title\"))[0].alias(\"title\"),\n",
    "    parse_title(col(\"title\"))[1].cast(\"integer\").alias(\"year\"),\n",
    "    split(\"genres\",\"\\|\").alias(\"genres\")\n",
    "    )\n",
    "\n",
    "movies_slv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_slv.where(col(\"year\").isNull()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some problems\n",
    "#movies_std.where(col(\"year\").isNull()).toPandas()\n",
    "movies_slv = movies_slv.withColumn(\"genres\",array_remove(col(\"genres\"),\"(no genres listed)\"))\n",
    "movies_slv.where(col(\"year\").isNull()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_slv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "ratings_slv = ratings_brz.select(\n",
    "    col(\"userId\").cast(\"bigint\"),\n",
    "    col(\"movieId\").cast(\"bigint\"),\n",
    "    col(\"rating\").cast(\"double\"),    \n",
    "    to_timestamp(from_unixtime(\"timestamp\")).alias(\"timestamp\")\n",
    ")\n",
    "ratings_slv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_slv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "links_slv = links_brz.select(\n",
    "    col(\"movieId\").cast(\"bigint\"),\n",
    "    col(\"imdbId\"),    \n",
    "    concat(lit(\"http://www.imdb.com/title/tt\"),col(\"imdbId\"),lit(\"/\")).alias(\"imdbUrl\"),\n",
    "    col(\"tmdbId\"),    \n",
    "    concat(lit(\"https://www.themoviedb.org/movie/\"),col(\"imdbId\"),lit(\"/\")).alias(\"tmdbUrl\")\n",
    ")\n",
    "links_slv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_slv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "trailers_slv = trailers_brz.select(\n",
    "    col(\"movieId\").cast(\"bigint\"),\n",
    "    col(\"youtubeId\"),    \n",
    "    concat(lit(\"https://www.youtube.com/embed/\"),col(\"youtubeId\"),lit(\"/\")).alias(\"youtubeUrl\")\n",
    ")\n",
    "trailers_slv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailers_slv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.4'></a>\n",
    "### 2.4 Write DataFrames to Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP SCHEMA IF EXISTS movielens CASCADE\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS movielens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(movies_slv.write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/movies/\")\n",
    "           .saveAsTable(\"movielens.movies\"))\n",
    "\n",
    "(ratings_slv.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/ratings/\")\n",
    "            .saveAsTable(\"movielens.ratings\"))\n",
    "\n",
    "(links_slv.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/links/\")\n",
    "          .saveAsTable(\"movielens.links\"))\n",
    "\n",
    "(trailers_slv.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"overwrite\")\n",
    "             .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/trailers/\")\n",
    "             .saveAsTable(\"movielens.trailers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "show databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "use movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select *\n",
    "from movielens.movies\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.5'></a>\n",
    "### 2.5 All at once\n",
    "\n",
    "Dividing your code in multiple cells is **NOT the way** you would do it for a production workload.<br/>\n",
    "We've been using variables, printing schema and data (toPandas and show) just to check our transformations are correct<br/>\n",
    "You would code the application like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP SCHEMA IF EXISTS movielens CASCADE\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS movielens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "@udf(\"array<string>\")\n",
    "def parse_title(t:str):\n",
    "    import re\n",
    "    titleRegex = re.compile(r'^(.+)\\((\\d{4})\\)$')\n",
    "    m = titleRegex.search(t.strip())\n",
    "    if m:\n",
    "        title,year= m.groups()\n",
    "        return [title.strip(),year.strip()]\n",
    "    else:\n",
    "        return [t,None]\n",
    "\n",
    "#movies\n",
    "(spark.read\n",
    "     .option(\"header\",\"true\")\n",
    "     .option(\"escape\",\"\\\"\")\n",
    "     .csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/movies/\")\n",
    "     .select(\n",
    "        col(\"movieId\").cast(\"bigint\"),\n",
    "        parse_title(col(\"title\"))[0].alias(\"title\"),\n",
    "        parse_title(col(\"title\"))[1].cast(\"integer\").alias(\"year\"),\n",
    "        split(\"genres\",\"\\|\").alias(\"genres\")\n",
    "     )\n",
    "     .write\n",
    "     .format(\"delta\") \n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/movies/\")\n",
    "     .saveAsTable(\"movielens.movies\")\n",
    ")\n",
    "  \n",
    "# ratings\n",
    "(spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/ratings/\")\n",
    "    .select(\n",
    "        col(\"userId\").cast(\"bigint\"),\n",
    "        col(\"movieId\").cast(\"bigint\"),\n",
    "        col(\"rating\").cast(\"double\"),    \n",
    "        to_timestamp(from_unixtime(\"timestamp\")).alias(\"timestamp\")\n",
    "    )\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/ratings/\")\n",
    "    .saveAsTable(\"movielens.ratings\")\n",
    ")\n",
    "\n",
    "#links\n",
    "(spark.read\n",
    "     .option(\"header\",\"true\")\n",
    "     .csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/links/\")\n",
    "     .select(\n",
    "        col(\"movieId\").cast(\"bigint\"),\n",
    "        col(\"imdbId\"),    \n",
    "        concat(lit(\"http://www.imdb.com/title/tt\"),col(\"imdbId\"),lit(\"/\")).alias(\"imdbUrl\"),\n",
    "        col(\"tmdbId\"),    \n",
    "        concat(lit(\"https://www.themoviedb.org/movie/\"),col(\"imdbId\"),lit(\"/\")).alias(\"tmdbUrl\"))\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/links/\")\n",
    "     .saveAsTable(\"movielens.links\")\n",
    ")\n",
    "\n",
    "#trailers\n",
    "(spark.read\n",
    "     .option(\"header\",\"true\")\n",
    "     .csv(f\"{hdfs_lakehouse_base_path}/bronze/movielens/trailers/\")\n",
    "     .select(\n",
    "        col(\"movieId\").cast(\"bigint\"),\n",
    "        col(\"youtubeId\"),    \n",
    "        concat(lit(\"https://www.youtube.com/embed/\"),col(\"youtubeId\"),lit(\"/\")).alias(\"youtubeUrl\"))\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"path\",f\"{hdfs_lakehouse_base_path}/silver/movielens/trailers/\")\n",
    "     .saveAsTable(\"movielens.trailers\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select *\n",
    "from movielens.movies\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Tear Down\n",
    "\n",
    "Once we complete the the lab we can stop all the services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop Hadoop\n",
    "Stop Hadoop <a href=\"http://localhost:2024/\">here </a>\n",
    "<p>\n",
    "<img style=\"width:48px\" src=\"https://cdn.iconscout.com/icon/free/png-256/free-hadoop-226007.png\" /> \n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
