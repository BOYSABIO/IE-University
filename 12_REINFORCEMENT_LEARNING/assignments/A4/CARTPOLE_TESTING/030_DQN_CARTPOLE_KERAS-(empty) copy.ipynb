{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb89376",
   "metadata": {},
   "source": [
    "### **CARTPOLE DQN KERAS**\n",
    "\n",
    "This is a naive version of the CARTPOLE DQN algorithm <br>\n",
    "Try to fill the missing parts\n",
    "\n",
    "This is the simplest approach of DQN with a single network (Remember the standard DQN uses 2 networks and trains one, in this one we use one and train one\n",
    "\n",
    "Fill the empty places and try to make it to work Ask for help if stocked!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557b1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use mixed-precision training for faster computations on supported GPUs\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "import sys\n",
    "sys.stderr = open('err.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2041146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "gpu_devices = [device for device in devices if device.device_type == 'GPU']\n",
    "for gpu in gpu_devices:\n",
    "    print('Using', gpu.physical_device_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d89820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2be125",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78010280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to fine tune\n",
    "# Try your own parameters\n",
    "# Remember epsilon and gamma are very important\n",
    "\n",
    "\n",
    "MAX_EPISODES = 300\n",
    "ROLLING_WINDOW = 20\n",
    "MEMORY_SIZE = 2000\n",
    "MAX_STEPS = 500\n",
    "\n",
    "gamma = 0.99                        # discount rate\n",
    "epsilon = 1.0                        # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "solved_threshold = 195\n",
    "\n",
    "verb = 0                             # to see traces (verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d058e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa84c05",
   "metadata": {},
   "source": [
    "#### **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5799f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# [CREATE YOUR NEURAL NETWORK TRY 16/32 or 24/24]\n",
    "###\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,), name=\"state_input\")\n",
    "    x = Dense(16, activation='relu', name=\"dense_1\")(inputs)\n",
    "    x = Dense(32, activation='relu', name=\"dense_2\")(x)\n",
    "    outputs = Dense(action_size, activation='linear', name=\"output_layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Q_Network\")\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fca383",
   "metadata": {},
   "source": [
    "#### **Support Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761e2bb",
   "metadata": {},
   "source": [
    "$Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') \\cdot (1 - \\text{done})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c07fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "def store(state, action, reward, next_state, done):             \n",
    "    \"\"\"\n",
    "    This function appends the actual observation and reward to the Replay buffer\n",
    "    \"\"\"\n",
    "    state = state.squeeze()\n",
    "    next_state = next_state.squeeze()\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def select_action_greedy(state, model, epsilon):           \n",
    "    \"\"\"\n",
    "    Selects action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    act_values = model.predict(state, verbose=verb)\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "# Sample experiences from the replay buffer\n",
    "def sample_experiences(batch_size):\n",
    "    \"\"\"\n",
    "    Samples a batch_size of experiences from the Replay buffer. \n",
    "    You MUST transform the data into numpy arrays as this accelerates the response time sensibily\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_states = np.array(next_states)\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "###\n",
    "# YOU MUST VECTIORIZE THE RETURN. TRANSFORM THE OUTPUTS IN np arrays otherwise it will be very slow\n",
    "###\n",
    "\n",
    "    \n",
    "def experience_replay(batch_size, model, epsilon):\n",
    "    \"\"\"\n",
    "    The critical function in the whole program\n",
    "    1. gets a minibatch from replay buffer\n",
    "    2. Predicts targets_qs from states (we need the full batch predicted but we'll avoid dones)\n",
    "    3. predicts next_qs from next_states\n",
    "    4. Bellman equation on next_qs obtains new target_qs $Q_{\\text{target}}(s_t, a_t) = r_t + \\gamma \\cdot \\max_{a'} Q(s_{t+1}, a') \\cdot (1 - \\text{done})$\n",
    "\n",
    "    6. Train DQN input states, output target_qs \n",
    "\n",
    "    \"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    \n",
    "    # Predict Q-values for current and next states using vectorized operations\n",
    "\n",
    "    \n",
    "#### \n",
    "#  [ Predict target_qs with model predict states, predict next_qs with model predict next states]\n",
    "####\n",
    "    target_qs = model.predict(states, verbose=verb) # Get current Q-values for all states\n",
    "    next_qs = model.predict(next_states, verbose=verb) # Get next Q-values for next states\n",
    "\n",
    "    # Update target Q-values using standard DQN logic   \n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * np.max(next_qs, axis=1) * (1 - dones)\n",
    "    \n",
    "    # Train the model on the Q-values\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "###\n",
    "#[Here you have a load and save weights. Can you make a breakpoint to restart the program after a while of training?]\n",
    "###\n",
    "\n",
    "def load(name, DQN):\n",
    "    DQN.load_weights(name)\n",
    "\n",
    "def save(name, DQN):\n",
    "    DQN.save_weights(name)\n",
    "\n",
    "DQN = build_model(state_size, action_size)\n",
    "DQN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loop for DQN\n",
    "rewards_per_episode = []\n",
    "rolling_avg_rewards = []\n",
    "rolling_avg = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action_greedy(state, DQN, epsilon)  # Updated to pass epsilon\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            experience_replay(batch_size, DQN, epsilon)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Track rewards\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "    rolling_avg_rewards.append(rolling_avg)\n",
    "    \n",
    "    print(f\"Episode: {episode+1:3}/{MAX_EPISODES}, Reward: {total_reward:+7.2f}, \"\n",
    "          f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:6.2f}, Steps: {step:3}\")\n",
    "    \n",
    "    # Check if environment is solved\n",
    "    if rolling_avg >= solved_threshold:\n",
    "        print(f\"Environment solved in {episode+1} episodes!\")\n",
    "        DQN.save(\"CartPole_dqn_model.keras\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Training completed in {training_duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5419d918",
   "metadata": {},
   "source": [
    "#### **Learning Plot and Episode Rewards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards with rolling average\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_per_episode, label='Rewards', color='blue')\n",
    "plt.plot(rolling_avg_rewards, label='Rolling Avg (Last 20 Episodes)', color='orange')\n",
    "plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "plt.title('DQN Training Performance CARTPOLE')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baef8f",
   "metadata": {},
   "source": [
    "#### **Simulation - Testing 10 episodes with the DQN Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 10 episodes\n",
    "start_time = time.time()\n",
    "\n",
    "for e_test in range(10):  # Run 10 test episodes\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    \n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the trained model for testing\n",
    "        action_vals = DQN.predict(state, verbose=0)  # Predict action values\n",
    "        action = np.argmax(action_vals[0])  # Choose the action with the highest Q-value\n",
    "\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps = steps + 1\n",
    "        if done or truncated:\n",
    "            print(f\"Test Episode: {e_test + 1:2}/10, Reward: {total_reward:.2f}, Steps: {steps:3}\")\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "testing_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Testing completed in {testing_duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f36b76",
   "metadata": {},
   "source": [
    "#### **Rendering 1 episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent with video rendering\n",
    "# This code is useful if you are using colab otherwise use render_mode='human'\n",
    "env = gym.make((\"CartPole-v1\"), render_mode='rgb_array')  # Enable RGB rendering\n",
    "frames = []  # Store frames for visualization\n",
    "\n",
    "# Render a single test episode\n",
    "state, _ = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "tot_rewards = 0\n",
    "\n",
    "while True:\n",
    "    # Use the trained model for action\n",
    "    action_vals = DQN.predict(state, verbose=0)  # Predict action values\n",
    "    action = np.argmax(action_vals[0])           # Choose the action with the highest Q-value\n",
    "\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    frames.append(env.render())                  # Save frame for rendering later\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    tot_rewards += reward\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        print(f\"Rendered Test Episode Reward: {tot_rewards:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save the rendered episode as a GIF\n",
    "def save_frames_as_gif(frames, path='./', filename='CARTPOLE_DQN.gif'):\n",
    "    images = [Image.fromarray(frame) for frame in frames]\n",
    "    gif_path = os.path.join(path, filename)\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=50, loop=0)\n",
    "    print(f\"Saved GIF to: {gif_path}\")\n",
    "\n",
    "save_frames_as_gif(frames, filename='CARTPOLE_DQN.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebfabb",
   "metadata": {},
   "source": [
    "## CartPole DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDQN Cartpole\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    inputs = Input(shape=(state_size,), name=\"state_input\")\n",
    "    x = Dense(24, activation='relu', name=\"dense_1\")(inputs)\n",
    "    x = Dense(24, activation='relu', name=\"dense_2\")(x)\n",
    "    outputs = Dense(action_size, activation='linear', name=\"output_layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"Q_Network\")\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Create main and target networks\n",
    "DQN = build_model(state_size, action_size)\n",
    "target_DQN = build_model(state_size, action_size)\n",
    "target_DQN.set_weights(DQN.get_weights()) # Initialize target network with main network weights\n",
    "\n",
    "def update_target_network():\n",
    "    \"\"\"\n",
    "    Update the target network with the main network weights\n",
    "    \"\"\"\n",
    "    target_DQN.set_weights(DQN.get_weights())\n",
    "\n",
    "def select_action_greedy(state, model, epsilon):\n",
    "    \"\"\"\n",
    "    Selects action using epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    act_values = model.predict(state, verbose=verb)\n",
    "    return np.argmax(act_values[0])\n",
    "\n",
    "def experience_replay(batch_size, model, epsilon):\n",
    "    \"\"\"\n",
    "    Sample a batch of experiences from the replay buffer.\n",
    "    Double DQN: Use the main network to select the action, and the target network to evaluate the Q-values.\n",
    "    \"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "\n",
    "    # Get current Q-values from the main network\n",
    "    target_qs = model.predict(states, verbose=verb)\n",
    "\n",
    "    # Double DQN: Use the main network to select the action, and the target network to evaluate the Q-values\n",
    "    next_qs_main = model.predict(next_states, verbose=verb)  # Main network predicts next states\n",
    "    next_qs_target = target_DQN.predict(next_states, verbose=verb)  # Target network predicts next states\n",
    "\n",
    "    # Select the best action for each next state using the main network\n",
    "    best_actions = np.argmax(next_qs_main, axis=1)\n",
    "\n",
    "    # Get Q-values for the best actions from the target network\n",
    "    best_qs = next_qs_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "    # Update the target Q-values using the Bellman equation\n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * best_qs * (1 - dones)\n",
    "\n",
    "    # Train the main network on the target Q-values\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "# Training loop for DDQN\n",
    "rewards_per_episode = []\n",
    "rolling_avg_rewards = []\n",
    "rolling_avg = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action = select_action_greedy(state, DQN, epsilon)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        store(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            experience_replay(batch_size, DQN, epsilon)\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Update target network every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        update_target_network()\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Track rewards\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "    rolling_avg_rewards.append(rolling_avg)\n",
    "    \n",
    "    print(f\"Episode: {episode+1:3}/{MAX_EPISODES}, Reward: {total_reward:+7.2f}, \"\n",
    "          f\"Epsilon: {epsilon:.2f}, Rolling Avg: {rolling_avg:6.2f}, Steps: {step:3}\")\n",
    "    \n",
    "    # Check if environment is solved\n",
    "    if rolling_avg >= solved_threshold:\n",
    "        print(f\"Environment solved in {episode+1} episodes!\")\n",
    "        DQN.save(\"CartPole_ddqn_model.keras\")\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "print(f\"Training completed in {training_duration:.2f} minutes\")\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_per_episode, label='Rewards', color='blue')\n",
    "plt.plot(rolling_avg_rewards, label='Rolling Avg (Last 20 Episodes)', color='orange')\n",
    "plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "plt.title('DDQN Training Performance CARTPOLE')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260156ee",
   "metadata": {},
   "source": [
    "## Testing Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: DQN_16-32\n",
      "Config: DQN_16-32, Ep:   1/300, Reward:   10.00, Epsilon: 0.99, Rolling Avg:  10.00\n",
      "Config: DQN_16-32, Ep:   2/300, Reward:   22.00, Epsilon: 0.98, Rolling Avg:  16.00\n",
      "Config: DQN_16-32, Ep:   3/300, Reward:   34.00, Epsilon: 0.97, Rolling Avg:  22.00\n",
      "Config: DQN_16-32, Ep:   4/300, Reward:   23.00, Epsilon: 0.96, Rolling Avg:  22.25\n",
      "Config: DQN_16-32, Ep:   5/300, Reward:   46.00, Epsilon: 0.95, Rolling Avg:  27.00\n",
      "Config: DQN_16-32, Ep:   6/300, Reward:   21.00, Epsilon: 0.94, Rolling Avg:  26.00\n",
      "Config: DQN_16-32, Ep:   7/300, Reward:   20.00, Epsilon: 0.93, Rolling Avg:  25.14\n",
      "Config: DQN_16-32, Ep:   8/300, Reward:   16.00, Epsilon: 0.92, Rolling Avg:  24.00\n",
      "Config: DQN_16-32, Ep:   9/300, Reward:   18.00, Epsilon: 0.91, Rolling Avg:  23.33\n",
      "Config: DQN_16-32, Ep:  10/300, Reward:   26.00, Epsilon: 0.90, Rolling Avg:  23.60\n",
      "Config: DQN_16-32, Ep:  11/300, Reward:   11.00, Epsilon: 0.90, Rolling Avg:  22.45\n",
      "Config: DQN_16-32, Ep:  12/300, Reward:   13.00, Epsilon: 0.89, Rolling Avg:  21.67\n",
      "Config: DQN_16-32, Ep:  13/300, Reward:   15.00, Epsilon: 0.88, Rolling Avg:  21.15\n",
      "Config: DQN_16-32, Ep:  14/300, Reward:   12.00, Epsilon: 0.87, Rolling Avg:  20.50\n",
      "Config: DQN_16-32, Ep:  15/300, Reward:   15.00, Epsilon: 0.86, Rolling Avg:  20.13\n",
      "Config: DQN_16-32, Ep:  16/300, Reward:   23.00, Epsilon: 0.85, Rolling Avg:  20.31\n",
      "Config: DQN_16-32, Ep:  17/300, Reward:   17.00, Epsilon: 0.84, Rolling Avg:  20.12\n",
      "Config: DQN_16-32, Ep:  18/300, Reward:   11.00, Epsilon: 0.83, Rolling Avg:  19.61\n",
      "Config: DQN_16-32, Ep:  19/300, Reward:   22.00, Epsilon: 0.83, Rolling Avg:  19.74\n",
      "Config: DQN_16-32, Ep:  20/300, Reward:   25.00, Epsilon: 0.82, Rolling Avg:  20.00\n",
      "Config: DQN_16-32, Ep:  21/300, Reward:   14.00, Epsilon: 0.81, Rolling Avg:  20.20\n",
      "Config: DQN_16-32, Ep:  22/300, Reward:   31.00, Epsilon: 0.80, Rolling Avg:  20.65\n",
      "Config: DQN_16-32, Ep:  23/300, Reward:   23.00, Epsilon: 0.79, Rolling Avg:  20.10\n",
      "Config: DQN_16-32, Ep:  24/300, Reward:   36.00, Epsilon: 0.79, Rolling Avg:  20.75\n",
      "Config: DQN_16-32, Ep:  25/300, Reward:   37.00, Epsilon: 0.78, Rolling Avg:  20.30\n",
      "Config: DQN_16-32, Ep:  26/300, Reward:   18.00, Epsilon: 0.77, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  27/300, Reward:   20.00, Epsilon: 0.76, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  28/300, Reward:   17.00, Epsilon: 0.75, Rolling Avg:  20.20\n",
      "Config: DQN_16-32, Ep:  29/300, Reward:   14.00, Epsilon: 0.75, Rolling Avg:  20.00\n",
      "Config: DQN_16-32, Ep:  30/300, Reward:   12.00, Epsilon: 0.74, Rolling Avg:  19.30\n",
      "Config: DQN_16-32, Ep:  31/300, Reward:   23.00, Epsilon: 0.73, Rolling Avg:  19.90\n",
      "Config: DQN_16-32, Ep:  32/300, Reward:   18.00, Epsilon: 0.72, Rolling Avg:  20.15\n",
      "Config: DQN_16-32, Ep:  33/300, Reward:   11.00, Epsilon: 0.72, Rolling Avg:  19.95\n",
      "Config: DQN_16-32, Ep:  34/300, Reward:   27.00, Epsilon: 0.71, Rolling Avg:  20.70\n",
      "Config: DQN_16-32, Ep:  35/300, Reward:    9.00, Epsilon: 0.70, Rolling Avg:  20.40\n",
      "Config: DQN_16-32, Ep:  36/300, Reward:   13.00, Epsilon: 0.70, Rolling Avg:  19.90\n",
      "Config: DQN_16-32, Ep:  37/300, Reward:   24.00, Epsilon: 0.69, Rolling Avg:  20.25\n",
      "Config: DQN_16-32, Ep:  38/300, Reward:   32.00, Epsilon: 0.68, Rolling Avg:  21.30\n",
      "Config: DQN_16-32, Ep:  39/300, Reward:   12.00, Epsilon: 0.68, Rolling Avg:  20.80\n",
      "Config: DQN_16-32, Ep:  40/300, Reward:   25.00, Epsilon: 0.67, Rolling Avg:  20.80\n",
      "Config: DQN_16-32, Ep:  41/300, Reward:   11.00, Epsilon: 0.66, Rolling Avg:  20.65\n",
      "Config: DQN_16-32, Ep:  42/300, Reward:   17.00, Epsilon: 0.66, Rolling Avg:  19.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === Global Settings ===\n",
    "MAX_EPISODES = 300\n",
    "ROLLING_WINDOW = 20\n",
    "MAX_STEPS = 500\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "solved_threshold = 195\n",
    "verb = 0\n",
    "\n",
    "# === Replay Buffer (will be resized per experiment) ===\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "# === Model Builder ===\n",
    "def build_model(state_size, action_size, network_structure=(16, 32), learning_rate=0.001):\n",
    "    inputs = Input(shape=(state_size,))\n",
    "    x = inputs\n",
    "    for i, units in enumerate(network_structure):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "    outputs = Dense(action_size, activation='linear')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# === Epsilon-Greedy Action Selection ===\n",
    "def select_action_greedy(state, model, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(model.output_shape[-1])\n",
    "    q_values = model.predict(state[np.newaxis, :], verbose=verb)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "# === Experience Replay Utilities ===\n",
    "def store(state, action, reward, next_state, done):\n",
    "    state = state.squeeze()\n",
    "    next_state = next_state.squeeze()\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
    "    batch = [replay_buffer[i] for i in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "def experience_replay(batch_size, model):\n",
    "    states, actions, rewards, next_states, dones = sample_experiences(batch_size)\n",
    "    target_qs = model.predict(states, verbose=verb)\n",
    "    next_qs = model.predict(next_states, verbose=verb)\n",
    "    target_qs[np.arange(batch_size), actions] = rewards + gamma * np.max(next_qs, axis=1) * (1 - dones)\n",
    "    model.fit(states, target_qs, epochs=1, verbose=0)\n",
    "\n",
    "def update_target_network(source_model, target_model):\n",
    "    target_model.set_weights(source_model.get_weights())\n",
    "\n",
    "# === Experiment Config Class ===\n",
    "class ExperimentConfig:\n",
    "    def __init__(self, network_structure=(16, 32), memory_size=2000,\n",
    "                 learning_rate=0.001, epsilon_decay=0.99,\n",
    "                 algorithm=\"DQN\", name=\"default\"):\n",
    "        self.network_structure = network_structure\n",
    "        self.memory_size = memory_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.algorithm = algorithm\n",
    "        self.name = name\n",
    "\n",
    "# === Run a Single Experiment ===\n",
    "def run_experiment(config):\n",
    "    global replay_buffer\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    DQN = build_model(state_size, action_size, config.network_structure, config.learning_rate)\n",
    "    target_DQN = build_model(state_size, action_size, config.network_structure, config.learning_rate) if config.algorithm == \"DDQN\" else None\n",
    "    if target_DQN: update_target_network(DQN, target_DQN)\n",
    "    \n",
    "    replay_buffer = deque(maxlen=config.memory_size)\n",
    "    epsilon = 1.0\n",
    "    rewards_per_episode = []\n",
    "    rolling_avg_rewards = []\n",
    "    episodes_to_solve = None\n",
    "    model_filename = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            action = select_action_greedy(state, DQN, epsilon)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            store(state, action, reward, next_state, done or truncated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                if config.algorithm == \"DDQN\":\n",
    "                    states, actions, rewards_, next_states, dones = sample_experiences(batch_size)\n",
    "                    target_qs = DQN.predict(states, verbose=verb)\n",
    "                    next_actions = np.argmax(DQN.predict(next_states, verbose=verb), axis=1)\n",
    "                    next_qs = target_DQN.predict(next_states, verbose=verb)\n",
    "                    target_qs[np.arange(batch_size), actions] = rewards_ + gamma * next_qs[np.arange(batch_size), next_actions] * (1 - dones)\n",
    "                    DQN.fit(states, target_qs, epochs=1, verbose=0)\n",
    "                else:\n",
    "                    experience_replay(batch_size, DQN)\n",
    "\n",
    "                if config.algorithm == \"DDQN\" and episode % 10 == 0:\n",
    "                    update_target_network(DQN, target_DQN)\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        epsilon = max(epsilon_min, epsilon * config.epsilon_decay)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        rolling_avg = np.mean(rewards_per_episode[-ROLLING_WINDOW:])\n",
    "        rolling_avg_rewards.append(rolling_avg)\n",
    "\n",
    "        print(f\"Config: {config.name}, Ep: {episode+1:3}/{MAX_EPISODES}, \"\n",
    "              f\"Reward: {total_reward:7.2f}, Epsilon: {epsilon:.2f}, \"\n",
    "              f\"Rolling Avg: {rolling_avg:6.2f}\")\n",
    "\n",
    "        if rolling_avg >= solved_threshold and episodes_to_solve is None:\n",
    "            episodes_to_solve = episode + 1\n",
    "            model_filename = f\"CartPole_{config.algorithm}_{config.name}_solved.keras\"\n",
    "            DQN.save(model_filename)\n",
    "            print(f\"Model saved as {model_filename}\")\n",
    "            break\n",
    "\n",
    "    training_duration = (time.time() - start_time) / 60\n",
    "    if model_filename is None:\n",
    "        model_filename = f\"CartPole_{config.algorithm}_{config.name}_final.keras\"\n",
    "        DQN.save(model_filename)\n",
    "        print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "    return {\n",
    "        'config': config,\n",
    "        'episodes_to_solve': episodes_to_solve,\n",
    "        'training_duration': training_duration,\n",
    "        'rewards': rewards_per_episode,\n",
    "        'rolling_avg': rolling_avg_rewards,\n",
    "        'model_filename': model_filename\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_model(results):\n",
    "    # Filter only solved models\n",
    "    solved_models = [r for r in results if r['episodes_to_solve'] is not None]\n",
    "    \n",
    "    if not solved_models:\n",
    "        print(\"No models reached the solved threshold!\")\n",
    "        return None\n",
    "    \n",
    "    # Sort by episodes to solve (ascending) and training duration (ascending)\n",
    "    best_model = min(solved_models, \n",
    "                    key=lambda x: (x['episodes_to_solve'], x['training_duration']))\n",
    "    \n",
    "    print(\"\\nBest Model Found:\")\n",
    "    print(\"----------------\")\n",
    "    print(f\"Configuration: {best_model['config'].name}\")\n",
    "    print(f\"Algorithm: {best_model['config'].algorithm}\")\n",
    "    print(f\"Network Structure: {best_model['config'].network_structure}\")\n",
    "    print(f\"Memory Size: {best_model['config'].memory_size}\")\n",
    "    print(f\"Learning Rate: {best_model['config'].learning_rate}\")\n",
    "    print(f\"Epsilon Decay: {best_model['config'].epsilon_decay}\")\n",
    "    print(f\"Episodes to Solve: {best_model['episodes_to_solve']}\")\n",
    "    print(f\"Training Duration: {best_model['training_duration']:.2f} minutes\")\n",
    "    print(f\"Model File: {best_model['model_filename']}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Define experiments\n",
    "experiments = [\n",
    "    # Network structure comparison (both DQN and DDQN)\n",
    "    ExperimentConfig(network_structure=(16, 32), algorithm=\"DQN\", name=\"DQN_16-32\"),\n",
    "    ExperimentConfig(network_structure=(24, 24), algorithm=\"DQN\", name=\"DQN_24-24\"),\n",
    "    ExperimentConfig(network_structure=(32, 32), algorithm=\"DQN\", name=\"DQN_32-32\"),\n",
    "    ExperimentConfig(network_structure=(16, 32), algorithm=\"DDQN\", name=\"DDQN_16-32\"),\n",
    "    ExperimentConfig(network_structure=(24, 24), algorithm=\"DDQN\", name=\"DDQN_24-24\"),\n",
    "    ExperimentConfig(network_structure=(32, 32), algorithm=\"DDQN\", name=\"DDQN_32-32\"),\n",
    "\n",
    "    # Replay buffer comparison (DDQN only)\n",
    "    ExperimentConfig(memory_size=1000, algorithm=\"DDQN\", name=\"DDQN_mem_1000\"),\n",
    "    ExperimentConfig(memory_size=2000, algorithm=\"DDQN\", name=\"DDQN_mem_2000\"),\n",
    "    ExperimentConfig(memory_size=5000, algorithm=\"DDQN\", name=\"DDQN_mem_5000\"),\n",
    "\n",
    "    # Learning rate comparison (DDQN only)\n",
    "    ExperimentConfig(learning_rate=0.0001, algorithm=\"DDQN\", name=\"DDQN_lr_0.0001\"),\n",
    "    ExperimentConfig(learning_rate=0.001, algorithm=\"DDQN\", name=\"DDQN_lr_0.001\"),\n",
    "    ExperimentConfig(learning_rate=0.01, algorithm=\"DDQN\", name=\"DDQN_lr_0.01\"),\n",
    "\n",
    "    # Epsilon decay comparison (DDQN only)\n",
    "    ExperimentConfig(epsilon_decay=0.95, algorithm=\"DDQN\", name=\"DDQN_eps_0.95\"),\n",
    "    ExperimentConfig(epsilon_decay=0.99, algorithm=\"DDQN\", name=\"DDQN_eps_0.99\"),\n",
    "    ExperimentConfig(epsilon_decay=0.995, algorithm=\"DDQN\", name=\"DDQN_eps_0.995\"),\n",
    "]\n",
    "\n",
    "# Run experiments and collect results\n",
    "results = []\n",
    "for config in experiments:\n",
    "    print(f\"\\nRunning experiment: {config.name}\")\n",
    "    result = run_experiment(config)\n",
    "    results.append(result)\n",
    "\n",
    "# Find and print the best model\n",
    "best_model = find_best_model(results)\n",
    "\n",
    "# Analyze results\n",
    "def plot_experiment_results(results):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot rewards for each experiment\n",
    "    for result in results:\n",
    "        plt.plot(result['rolling_avg'], label=f\"{result['config'].name}\")\n",
    "    \n",
    "    plt.axhline(y=solved_threshold, color='red', linestyle='--', label='Solved Threshold')\n",
    "    plt.title('Comparison of Different Configurations')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rolling Average Reward')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nExperiment Results Summary:\")\n",
    "print(\"-------------------------\")\n",
    "for result in results:\n",
    "    config = result['config']\n",
    "    print(f\"\\nConfiguration: {config.name}\")\n",
    "    print(f\"Algorithm: {config.algorithm}\")\n",
    "    print(f\"Network Structure: {config.network_structure}\")\n",
    "    print(f\"Memory Size: {config.memory_size}\")\n",
    "    print(f\"Learning Rate: {config.learning_rate}\")\n",
    "    print(f\"Epsilon Decay: {config.epsilon_decay}\")\n",
    "    print(f\"Episodes to Solve: {result['episodes_to_solve']}\")\n",
    "    print(f\"Training Duration: {result['training_duration']:.2f} minutes\")\n",
    "    print(f\"Model File: {result['model_filename']}\")\n",
    "\n",
    "# Plot results\n",
    "plot_experiment_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
