{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56f3b7e",
   "metadata": {},
   "source": [
    "# Naive Bayes for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b87310-0687-4000-95c2-c680a1aca7c6",
   "metadata": {},
   "source": [
    "Example to detect spam sms\n",
    "<BR> This exercise makes use of frequent text processes when using a NB model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0165e",
   "metadata": {},
   "source": [
    "### Read data: a known repository of sms spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76203e8-3d02-46e7-95bc-a472d791f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51adefc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message\n",
       "0      ham                      Ok lar... Joking wif u oni...\n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2      ham  U dun say so early hor... U c already then say...\n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "...    ...                                                ...\n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5567   ham               Will ü b going to esplanade fr home?\n",
       "5568   ham  Pity, * was in mood for that. So...any other s...\n",
       "5569   ham  The guy did some bitching but I acted like i'd...\n",
       "5570   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5571 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! pip install requests\n",
    "import requests \n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "data_file = 'SMSSpamCollection'\n",
    "\n",
    "# Make request\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Get filename\n",
    "filename = url.split('/')[-1]\n",
    "\n",
    "# Download zipfile\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(resp.content)\n",
    "\n",
    "# Extract Zip\n",
    "with zipfile.ZipFile(filename, 'r') as zip:\n",
    "  zip.extractall('')\n",
    "\n",
    "# Read Dataset\n",
    "data = pd.read_table(data_file, \n",
    "                     header = 0,\n",
    "                     names = ['type', 'message']\n",
    "                     )\n",
    "\n",
    "# Show dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e930345",
   "metadata": {},
   "source": [
    "#### Install the language toolkit for the text processes and everything else necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46e7d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SLO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SLO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# NLTK: Natural Language Toolkit\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8d218",
   "metadata": {},
   "source": [
    "**1st step: Tokenization**. <BR> It consists of separating the messages into words in order to be able to treat each. <BR>\n",
    "At the same time, we remove punctuation (thanks to the RegexpTokenizer function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ec412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, don, t, think, he, goes, to, usf, he,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[FreeMsg, Hey, there, darling, it, s, been, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[This, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>[Will, ü, b, going, to, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[Pity, was, in, mood, for, that, So, any, othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[The, guy, did, some, bitching, but, I, acted,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[Rofl, Its, true, to, its, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                        [Ok, lar, Joking, wif, u, oni]  \n",
       "1     [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "2     [U, dun, say, so, early, hor, U, c, already, t...  \n",
       "3     [Nah, I, don, t, think, he, goes, to, usf, he,...  \n",
       "4     [FreeMsg, Hey, there, darling, it, s, been, 3,...  \n",
       "...                                                 ...  \n",
       "5566  [This, is, the, 2nd, time, we, have, tried, 2,...  \n",
       "5567       [Will, ü, b, going, to, esplanade, fr, home]  \n",
       "5568  [Pity, was, in, mood, for, that, So, any, othe...  \n",
       "5569  [The, guy, did, some, bitching, but, I, acted,...  \n",
       "5570                   [Rofl, Its, true, to, its, name]  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize: create words from sentences, and removes punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['tokens'] = data.apply(lambda x: tokenizer.tokenize(x['message']), axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508d4df",
   "metadata": {},
   "source": [
    "**2nd step: Elimination of stop words**: <BR>Elimination of words that normally do not add value (prepositions, conjunctions, etc.), only noise (there are exceptions, though) <BR> This is done to reduce size of our dataset (already large). \n",
    "<BR> There are many sources of stopwords in Python, for English language, Spacy has 326 words, Gensim 337, Scikit-learn 318, NLTK library, 179. \n",
    "<BR> More or less words is not better in itself, you need to read them and assess whether you want to include or remove some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3b9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 stop words provided by Sklearn: \n",
      "\n",
      "frozenset({'re', 'behind', 'thence', 'even', 'detail', 'keep', 'hundred', 'fire', 'anyone', 'your', 'own', 'beforehand', 'against', 'at', 'via', 'besides', 'which', 'many', 'whatever', 'though', 'nowhere', 'anyway', 'cant', 'please', 'more', 'so', 'none', 'should', 'amount', 'thru', 'ten', 'otherwise', 'must', 'five', 'un', 'already', 'this', 'somehow', 'seem', 'my', 'throughout', 'herein', 'how', 'we', 'yourselves', 'whereafter', 'however', 'among', 'much', 'becomes', 'themselves', 'toward', 'whole', 'us', 'on', 'he', 'been', 'con', 'hence', 'else', 'therein', 'the', 'thereby', 'when', 'am', 'off', 'anywhere', 'or', 'another', 'describe', 'very', 'hereafter', 'two', 'per', 'in', 'these', 'become', 'both', 'too', 'hasnt', 'since', 'onto', 'but', 'mostly', 'for', 'were', 'nine', 'full', 'sometime', 'are', 'towards', 'will', 'be', 'everything', 'may', 'top', 'nevertheless', 'can', 'except', 'part', 'under', 'three', 'most', 'whereupon', 'bottom', 'front', 'often', 'back', 'with', 'one', 'only', 'fill', 'no', 'him', 'six', 'herself', 'they', 'forty', 'across', 'fifteen', 'our', 'rather', 'anyhow', 'afterwards', 'here', 'while', 'about', 'empty', 'wherever', 'other', 'those', 'get', 'go', 'never', 'few', 'sincere', 'twenty', 'and', 'has', 'something', 'whereby', 'almost', 'whence', 'ltd', 'between', 'thus', 'latter', 'wherein', 'together', 'done', 'some', 'meanwhile', 'neither', 'side', 'thereupon', 'third', 'noone', 'least', 'amongst', 'that', 'co', 'seemed', 'ourselves', 'hers', 'thin', 'well', 'its', 'yet', 'who', 'everywhere', 'nothing', 'after', 'thick', 'what', 'it', 'have', 'up', 'without', 'a', 'anything', 'everyone', 'see', 'serious', 'show', 'during', 'inc', 'mill', 'same', 'somewhere', 'such', 'was', 'next', 'either', 'indeed', 'do', 'myself', 'bill', 'became', 'than', 'around', 'all', 'from', 'his', 'i', 'take', 'whoever', 'being', 'latterly', 'couldnt', 'give', 'whither', 'beside', 'former', 'cannot', 'twelve', 'me', 'nobody', 'first', 'put', 'of', 'upon', 'also', 'less', 'any', 'yours', 'eight', 'to', 'could', 'others', 'whereas', 'several', 'de', 'then', 'into', 'yourself', 'beyond', 'below', 'mine', 'whom', 'cry', 'ie', 'might', 'an', 'why', 'she', 'every', 'would', 'someone', 'name', 'not', 'seems', 'therefore', 'out', 'through', 'again', 'eg', 'you', 'further', 'moreover', 'above', 'her', 'there', 'due', 'had', 'once', 'becoming', 'along', 'before', 'hereby', 'elsewhere', 'by', 'etc', 'four', 'although', 'is', 'over', 'them', 'whose', 'interest', 'ever', 'system', 'thereafter', 'sixty', 'hereupon', 'namely', 'where', 'seeming', 'still', 'fifty', 'enough', 'down', 'himself', 'last', 'their', 'as', 'perhaps', 'eleven', 'always', 'formerly', 'made', 'whenever', 'ours', 'if', 'until', 'sometimes', 'each', 'within', 'amoungst', 'call', 'move', 'now', 'because', 'whether', 'itself', 'found', 'nor', 'alone', 'find'})\n"
     ]
    }
   ],
   "source": [
    "# These are the stop words provided by Sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "skstop = text.ENGLISH_STOP_WORDS\n",
    "print(len(skstop), \"stop words provided by Sklearn: \")\n",
    "print()\n",
    "print(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a495b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 stop words provided by nltk: \n",
      "\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# These are the stop words provided by nltk, the library we will be using in this example\n",
    "from nltk.corpus import stopwords\n",
    "nltkstop=stopwords.words('english')\n",
    "print(len(nltkstop), \"stop words provided by nltk: \")\n",
    "print()\n",
    "print(nltkstop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7b8b5-5a99-4939-9757-10895212b37d",
   "metadata": {},
   "source": [
    "We will use the nltk list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9d03f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[FreeMsg, Hey, darling, 3, week, word, back, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[This, 2nd, time, tried, 2, contact, u, U, 750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>[Will, ü, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[Pity, mood, So, suggestions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[The, guy, bitching, I, acted, like, intereste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[Rofl, Its, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                        [Ok, lar, Joking, wif, u, oni]  \n",
       "1     [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "2         [U, dun, say, early, hor, U, c, already, say]  \n",
       "3     [Nah, I, think, goes, usf, lives, around, though]  \n",
       "4     [FreeMsg, Hey, darling, 3, week, word, back, I...  \n",
       "...                                                 ...  \n",
       "5566  [This, 2nd, time, tried, 2, contact, u, U, 750...  \n",
       "5567           [Will, ü, b, going, esplanade, fr, home]  \n",
       "5568                      [Pity, mood, So, suggestions]  \n",
       "5569  [The, guy, bitching, I, acted, like, intereste...  \n",
       "5570                            [Rofl, Its, true, name]  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [item for item in x if item not in nltkstop])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1916c-820a-45fe-95bb-b87450e4e885",
   "metadata": {},
   "source": [
    "**3rd step: Apply stemming & lemmatization.** \n",
    "\n",
    "**Stemming**: Stemming is a process of reducing inflected words to their word stem, base or root form. This is done by removing affixes, which are word parts that are added to the root to change its meaning or grammatical function. For example, the word \"running\" can be stemmed to \"run\" by removing the suffix \"-ing\".\n",
    "\n",
    "**Lemmatization**: It is useful in NLP whenever we are working with a language in which there are words with similar meaning and different spelling:  for instance (Enlgish): good (good), better (better) and the best (best) are different words, close meanings. In these cases stemming would not work. Thus, the lemmatization would convert all those words to their base (good), in such a way that they come to mean the same thing. \n",
    "\n",
    "The nltk does not perform the conversion automatically unless it is given additional information about the type of word (adjective, noun, verb, etc). If you do not specify the grammatical category, the lemmatiser assumes that the words are nouns, and for nouns there is no direct association, for instance, between \"better\" and \"good\". This makes more complex the whole process, so we would not implement lemmatization in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe38b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several alternatives for stemming, we are applying SnowballStemmer (less aggresive than others, still with some defects)\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [stemmer.stem(item) for item in x])\n",
    "data\n",
    "data.to_excel(\"data_stemming.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b265bb-effd-4e32-a07a-b5c3cd3b1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lemmatization: the following library is better option than nltk in lemmatization\n",
    "# !pip install spacy \n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_spacy(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(lemmatize_spacy)\n",
    "data\n",
    "data.to_excel(\"data_lemmatization.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f0a6a",
   "metadata": {},
   "source": [
    "## Naive Bayes classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4718e8-2a58-451b-b7b8-518ceb9f5563",
   "metadata": {},
   "source": [
    "Once we have processed our text, we build our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd1a7b",
   "metadata": {},
   "source": [
    "Naive Bayes model admits two types of source data:<BR>\n",
    "\n",
    "**1. A Matrix** that shows, for each document, either how many times each of the words in all the documents has appeared or how important is the word in each document compared to the whole source of data.  <BR>\n",
    "**2. An array of appearances**. It is similar to a TF matrix, but in this case, instead of indicating the number of occurrences, it simply indicates whether or not that word appeared.\n",
    "<BR>If we have plenty of data, normally a Matrix is preferred, our choice in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12c21",
   "metadata": {},
   "source": [
    "First, we are detokenizing the rows, that is, joining back the words, once they have been cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40f2e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah I think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darl 3 week word back I like fun s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>this 2nd time tri 2 contact u u 750 pound priz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>will ü b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>piti mood so suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>the guy bitch I act like interest buy someth e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl it true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                                 ok lar joke wif u oni  \n",
       "1     free entri 2 wkli comp win fa cup final tkts 2...  \n",
       "2                   u dun say earli hor u c alreadi say  \n",
       "3                nah I think goe usf live around though  \n",
       "4     freemsg hey darl 3 week word back I like fun s...  \n",
       "...                                                 ...  \n",
       "5566  this 2nd time tri 2 contact u u 750 pound priz...  \n",
       "5567                       will ü b go esplanad fr home  \n",
       "5568                               piti mood so suggest  \n",
       "5569  the guy bitch I act like interest buy someth e...  \n",
       "5570                                  rofl it true name  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unify the strings once again\n",
    "data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3ab3e",
   "metadata": {},
   "source": [
    "Now we are splitting our data as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e488317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Make split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data['tokens'], \n",
    "    data['type'], \n",
    "    test_size= 0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "681c3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size:  4456\n",
      "Testing data set size:  1115\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data set size: \", len(x_train))\n",
    "print(\"Testing data set size: \", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad2d69",
   "metadata": {},
   "source": [
    "And here is where we create the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa9c5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents = 'ascii', \n",
    "    lowercase = True\n",
    "    )\n",
    "\n",
    "# Fit vectorizer & transform it\n",
    "vectorizer_fit = vectorizer.fit(x_train)\n",
    "x_train_transformed = vectorizer_fit.transform(x_train)\n",
    "x_test_transformed = vectorizer_fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35ee4da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 9458 stored elements and shape (1115, 6387)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f099b",
   "metadata": {},
   "source": [
    "Now we build the Naive Bayes model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2846dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from sklearn.naive_bayes import MultinomialNB # Multinomial is adecquate for discrete data (counting of events, for instance)\n",
    "\n",
    "# Train the model\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes_fit = naive_bayes.fit(x_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f12113",
   "metadata": {},
   "source": [
    "And we make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d41a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "train_predict = naive_bayes_fit.predict(x_train_transformed)\n",
    "test_predict = naive_bayes_fit.predict(x_test_transformed)\n",
    "\n",
    "def get_scores(y_real, predict):\n",
    "  ba_train = balanced_accuracy_score(y_real, predict)\n",
    "  return ba_train\n",
    "\n",
    "def print_scores(scores):\n",
    "  return f\"Balanced Accuracy: {scores}\"\n",
    "\n",
    "train_scores = get_scores(y_train, train_predict)\n",
    "test_scores = get_scores(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600829ba",
   "metadata": {},
   "source": [
    "This is the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a12d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Train Accuracy\n",
      "Balanced Accuracy: 98.43%\n",
      "\n",
      "## Test Accuracy\n",
      "Balanced Accuracy: 96.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"## Train Accuracy\")\n",
    "print(print_scores(f\"{train_scores:.2%}\"))\n",
    "print(\"\\n## Test Accuracy\")\n",
    "print(print_scores(f\"{test_scores:.2%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03e905",
   "metadata": {},
   "source": [
    "And the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b937c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "mat = confusion_matrix(y_test, test_predict)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=naive_bayes_fit.classes_, yticklabels=naive_bayes_fit.classes_)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
