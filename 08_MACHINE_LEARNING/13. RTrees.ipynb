{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # Increase cell width\n",
    "display(HTML(\"<style>.rendered_html { font-size: 16px; }</style>\")) # Increase font size\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The exercise has been taken from the book \"Introduction to Statistical Learning\" (https://www.amazon.es/Introduction-Statistical-Learning-Applications-Statistics/dp/3031387465/ref=tmm_hrd_swatch_0?_encoding=UTF8&sr=8-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice #2\n",
    "\n",
    "Regression trees with Boston dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this practice we will make use of the Boston Dataset, which was included into the sklearn datasets (it is discontinued as it was catalogued as non-ethical).\n",
    "<BR>\n",
    "It consists of 506 rows and 14 columns. The goal is to predict the **MEDV** variable.\n",
    "\n",
    "- CRIM - per capita crime rate by town\n",
    "- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS - proportion of non-retail business acres per town.\n",
    "- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- NOX - nitric oxides concentration (parts per 10 million)\n",
    "- RM - average number of rooms per dwelling\n",
    "- AGE - proportion of owner-occupied units built prior to 1940\n",
    "- DIS - weighted distances to five Boston employment centres\n",
    "- RAD - index of accessibility to radial highways\n",
    "- TAX - full-value property-tax rate per \\$10,000\n",
    "- PTRATIO - pupil-teacher ratio by town\n",
    "- AFRO - 1000(Bk - 0.63)^2 where Bk is the proportion of Afroamerican by town\n",
    "- LSTAT - % lower status of the population\n",
    "- **MEDV - Median value of owner-occupied homes in $1000's**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset and split it into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# Red data\n",
    "filename = 'boston.xlsx'\n",
    "df = pd.read_excel(filename, index_col=0) #it has two sheets, we load the 2nd one\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not the scope of this practice to focus on other things rather than the Decision Trees themselves. So I leave to you as future work the inspecting of the dataset profiling and the feature engineering process.\n",
    "\n",
    "Since there are not null values I will just divide the dataframe into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_df(dataframe, seed=None, percentage=0.8):\n",
    "    \n",
    "    X = dataframe.loc[:, dataframe.columns != 'MEDV']\n",
    "    y = dataframe['MEDV']\n",
    "\n",
    "    return train_test_split(X, y, test_size=1-percentage, random_state=seed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_df(df, seed=42, percentage=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our first decision tree. The basic DT is implemented in sklearn in the DecisionTreeRegressor method. \n",
    "<BR>You can take a look to its configuration and hyperparameters in the following link: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "For the practice we will use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "boston_tree = DecisionTreeRegressor(random_state=42) \n",
    "boston_tree.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tree uses the squared_error (MSE), although it penalizes larger errors more, since it squares them.\n",
    "<BR>The tree searches for the split that minimizes the MSE in the child nodes (minimizing the variance).\n",
    "<BR>\n",
    "<BR>(If we have outliers, one of the alternatives is to use the absolute_error (MAE), which is more robust to outliers because it penalizes errors linearly).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "predictions = boston_tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE = {0:.4f}\".format(mse))\n",
    "\n",
    "mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "print(f\"MAPE = {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make sense of the quality of the predictions by plotting them against the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(y_test,boston_tree.predict(X_test),color='blue',label='Prediction DT')\n",
    "min_val, max_val = min(y_test.min(), boston_tree.predict(X_test).min()), max(y_test.max(), boston_tree.predict(X_test).max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label=\"Perfect Fit (y = x)\")\n",
    "plt.xlabel(\"Actual Values\", fontsize=16)\n",
    "plt.ylabel(\"Predictions\", fontsize=16)\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a look to the tree itself. We will need to make use of an external library: `pydotplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pydotplus\n",
    "\n",
    "# ! pip install graphviz \n",
    "# you may need to install this library directly from https://graphviz.gitlab.io/_pages/Download/Download_windows.html \n",
    "# and then uncomment following two lines\n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38/bin/'(installation folder)\n",
    "\n",
    "from io import StringIO\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "def plot_tree(tree, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree, out_file=dot_data, feature_names=feature_names,\n",
    "                    filled=True, rounded=True,special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png(\"boston_train.png\")\n",
    "    return Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(boston_tree, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many nodes and leaves?\n",
    "print(\"Number of nodes: \", boston_tree.tree_.node_count)\n",
    "print(\"Number of leaves: \", boston_tree.get_n_leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two aspects can be highlighted after taking a look at the tree:\n",
    " - The tree is huge! As we have not set any complexity pruning or max_depth we have allowed the tree to grow without any limit\n",
    " - `RM` and `LSTAT` seem to be the most important features in order to predict the value of the houses. The variable `RM` measures the number of rooms (i.e., the size of the house). The tree indicates that larger houses correspond to more expensive houses. The `LSTAT` (the percentage of individuals with lower socioeconomic status) indicates that houses in expensive neighborhoods are more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this later point by plotting the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "importances = boston_tree.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.bar(X_train.columns[indices], importances[indices]) #average reduction in MSE resulting from splitting at each node of the tree using that feature\n",
    "plt.title('Feature Importance', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will prune the tree to see if we can improve performance.\n",
    "\n",
    "There are different Pruning Parameters:\n",
    "\n",
    " - max_leaf_nodes: Reduce the number of leaf nodes\n",
    " - min_samples_leaf: Restrict the size of sample leaf. Minimum sample size in terminal nodes can be fixed to a specific value or a % of total \n",
    " - max_depth: Reduce the depth of the tree to build a generalized tree. \n",
    "\n",
    "Let's focus on the depth of the tree. We will test different depth thresholds via CV (k-fold) by using the `GridSearchCV` provided by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': range(1,16)} # 15 different depth levels\n",
    "\n",
    "boston_tree_pruned_cv = GridSearchCV(boston_tree, \n",
    "                   param_grid,\n",
    "                   scoring='neg_mean_squared_error', #negative since scikit always look to maximize metrics\n",
    "                   cv=5 , n_jobs=1, verbose=1)\n",
    "\n",
    "boston_tree_pruned_cv.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on train set:\")\n",
    "print()\n",
    "print(boston_tree_pruned_cv.best_params_)\n",
    "print()\n",
    "print(\"Grid scores:\")\n",
    "\n",
    "means = boston_tree_pruned_cv.cv_results_['mean_test_score']\n",
    "stds = boston_tree_pruned_cv.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, boston_tree_pruned_cv.cv_results_['params']):\n",
    "    print(\"MSE = %0.3f (+/%0.03f) for %r\" % (-mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "plt.errorbar(range(1,16,1), [-m for m in means], yerr=stds, fmt='-o')\n",
    "plt.title('MSE for different Depths', fontsize=20)\n",
    "plt.xlabel(\"Depth\", fontsize=16)\n",
    "plt.ylabel(\"MSE\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the plot, the optimal value for the depth of the decision tree is 10. However, we can find a local optimal at depth 6 (the difference does not appear to be high and the complexity is much reduced). <BR>Let's prune the tree with this value (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_tree_pruned = DecisionTreeRegressor(random_state=42, max_depth=6)\n",
    "tree=boston_tree_pruned.fit(X_train, y_train)\n",
    "predictions = boston_tree_pruned.predict(X_test)\n",
    "mse_pruned = mean_squared_error(y_test, predictions)\n",
    "print(f\"MSE = {mse_pruned:.2f} vs. {mse:.2f} on the non-pruned tree\")\n",
    "\n",
    "mape_pruned = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
    "print(f\"MAPE = {mape_pruned:.2f}% vs. {mape:.2f}% on the non-pruned tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have reduced the error with a smaller tree. Let's plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many nodes and leaves?\n",
    "print(\"Number of nodes: \", boston_tree_pruned.tree_.node_count)\n",
    "print(\"Number of leaves: \", boston_tree_pruned.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "text_representation = tree.export_text(boston_tree_pruned, feature_names=list(X_test.columns))\n",
    "print(text_representation)\n",
    "\n",
    "with open(\"decistion_tree.log\", \"w\") as fout:\n",
    "    fout.write(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize our pruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "# DOT data\n",
    "dot_data = tree.export_graphviz(boston_tree_pruned, out_file=None, \n",
    "                                feature_names=X_train.columns,  \n",
    "                                filled=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it into a high resolution image\n",
    "graph.render(\"boston_pruned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
